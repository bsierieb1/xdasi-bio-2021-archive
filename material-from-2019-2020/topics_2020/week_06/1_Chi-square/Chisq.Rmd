---
title: "Chi-sq and proportional test"
author: "Manpreet S. Katari / Kris Gunsalus"
output:
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 3
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background Reading

+ Whitlock, Chapters 6, 7, & 8 

## $\chi^{2}$ - goodness of fit test

The $\chi^{2}$ goodness of fit test is simply looking at the difference between experimentally `observed` data values and the `expected` values from a model.

## Chi-squared Test

The **Chi-squared Test** statistic, $X^2$, compares individual proportions in each group to the **expected proportion** based on the **population mean estimate**.

The test statistic, $X^2$, is:

$$ X^2 = \sum\frac{(Obs-Exp)^2}{Exp}$$

So for each cell, we take the square of the difference between the Observed and Expected and divide by Expected, then sum these to get $X^2$. Under $H_0$, $X^2$ will have an asymptotic $\chi^2$ distribution with $(r-1)(c-1) = 1$ degree of freedom.

## $\chi^{2}$ distribution

A $\chi^{2}$ distribution is when you take the sum of squared random values selected from a normal distribution. The degree of freedom, $k$ is simply the number of random values.

$$Q = \sum\limits_{i=1}^{k} Z_{i}^{2}$$

Let's simulate this data to see what it looks like when k = 10, 5, and 1 and the values are retrieved from a standard normal distribution.

```{r}
#################

# Sample size k = 10 ==> Chi-squared degrees of freedom = 10
chik=10
normval=numeric()
for (i in 1:10000) {  # repeat this 10000 times
  
  # Randomly sample k items from the standard normal distribution
  # then take the sum of squared values of the random samples
  normval[i]=sum(rnorm(chik)**2)
}

# make a histogram of the results
hist(normval, breaks=100, # make 100 bins on the x-axis
     col=rgb(1,0,0,.25),  # make it red, with 25% transparency
                          # this allows us to show multiple histograms on the same plot
     ylim=c(0,2000),      # set the limits of the y-axis
     main = "Chi-square samples from the standard normal dist"
    )
# Sample size k = 5 ==> Chi-squared degrees of freedom = 5
chik=5
normval=numeric()
for (i in 1:10000) {
  normval[i]=sum(rnorm(chik)**2)
}

# the `add` directive adds the new histogram to the old plot
hist(normval, breaks=75,          # reduce the number of bins since this distribution
                                  # is squished to the left
     col=rgb(0,1,0,.25), add=T)   # green, with 25% transparency

# Sample size k = 1 ==> Chi-squared degrees of freedom = 1
chik=1
normval=numeric()
for (i in 1:10000) {
  normval[i]=sum(rnorm(chik)**2)
}

hist(normval, breaks=50,          # use even fewer bins
     col=rgb(0,0,1,.25), add=T)   # blue, with 25% transparency
```

It is clear to see that as $k$ increases it begins to start looking like a normal distribution. Also, this only happens when the **sample size is at least 10**, which is why it is not recommended to use $\chi^{2}$ test for small values of $k$ ( less than 10 ).


## Example: Two-way comparison

An international study was done to test the hypothesis that women who postpone childbirth until later in life have a higher risk of breast cancer, which they examined by asking whether there is an association between the incidence of breast cancer and the age at which women first gave birth (Rosner, Example 10.4). 

The study found that 683 out of 3,220 women WITH breast cancer first gave birth above the age of 30 (21.2%), whereas 1,498 out of 10,245 women WITHOUT breast cancer first gave birth at an age above 30 (14.6%):

Is this difference significant? If there is no association, then the probabilities of these two factors should be independent, and the expected proportion of individuals with both characteristics would just equal the product of the independent proportions. For example:

$$ \hat{p}_{(Cancer\ \cap Above30)} = \hat{p}_{Cancer} * \hat{p}_{Above30} $$

This is our **null hypothesis**. Below we will consider three different approaches to determine the significance of the differences.

## Contigency tables

Categorical data are typically organized into a 2x2 **contigency table** to represent data for two variables with two possible outcomes each. Larger tables can be used for multinomial vs. binary outcomes.

**Note:** _**By convention**_, the two samples to be compared are presented in the ROWs (e.g. Case, Control), and the different groups within each sample are presented in the COLUMNs (e.g. Above30, Below30). However, _**table orientation is arbritrary for the purpose of the statistical tests**_, which will give the same result either way.

Let's make a contingency table with two rows and two columns to represent our cancer data. 

+ **Row 1:** women WITH cancer ("Case")
+ **Row 2:** women WITHOUT cancer ("Control")
+ **Col 1:** women first giving birth above 30 ("Above30")
+ **Col 2:** women first giving birth below 30 ("Below30")

A common practice is to designate rows with the subscript $i$ and columns with the subscript $j$, so the counts in each cell will have an index $x_{ij}$.

Since this can get confusing, we can alternatively refer to the quadrants as $a$ (top left), $b$ (top right), $c$ (bottom left), and $d$ (bottom right).

+ $a = x_{11}$ = # of women with cancer & first birth ABOVE 30
+ $b = x_{12}$ = # of women with cancer & first birth BELOW 30
+ $c = x_{21}$ = # of women without cancer & first birth ABOVE 30
+ $d = x_{22}$ = # of women without cancer & first birth BELOW 30

Individual row and column totals are called _**marginal**_ totals (since the sum of the cells is written in the margins). The row margins will be $m_1 = (a+b)$ and $m_2 = (c+d)$, and the column margins $n_1 = (a+c)$ and $n_2 = (b+d)$:

+ **Row 1 margin:** $m_1 = x_{1+} = x_{11} + x_{12}$ = all women with cancer
+ **Row 2 margin:** $m_2 = x_{2+} = x_{21} + x_{22}$ = all women without cancer
+ **Col 1 margin:** $n_1 = x_{+1} = x_{11} + x_{21}$ = # of women who first gave birth over 30
+ **Col 2 margin:** $n_2 = x_{+2} = x_{12} + x_{22}$ = # of women who first gave birth under 30

Finally, the _**grand total**_ is:

+ **Grand total:** $N = (a+b+c+d) = x_{11} + x_{12} + x_{21} + x_{22} = x_{1+} + x_{2+} = x_{+1} + x_{+2}$ 

Now let's generate the contingency table:

```{r}
# from above
Case   = c(683,2537)   # total =  3220
Control= c(1498,8747)  # total = 10245

# contingency table with row and col margins, plus grand total
CaseT     = c(Case,    sum(Case))     # = c( 683, 2537,  3220)
ControlT  = c(Control, sum(Control))  # = c(1498, 8747, 10245) 
Totals    = CaseT + ControlT

data_table = rbind(CaseT, ControlT, Totals)
colnames(data_table) = c("Above30", "Below30", "Total")
rownames(data_table) = c("Case", "Control", "Total")
knitr::kable(data_table)
```
under the null hypothesis $H_0$, the proportion of women with first birth over 30 in each group (Case and Control) should be the same, and the joint probability should follow the product rule under independence. We can restate our null hypothesis above more generally as:

$$H_{0}: \hat{p}_{ij} = \hat{p}_{i+}*\hat{p}_{+j}$$

We first determine the *Expected* values for each cell, which will be the total table count multipled by the the joint probability under independence. This can also be calculated as the product of the *row margins* and the *column margins*, divided by the grand total:

$$H_{0}: E_{ij} = N*\hat{p}_{i+}*\hat{p}_{+j} = \frac{x_{i+}x_{+j}}{N} = \frac{m_in_j}{N}$$

So for example, 

$$E(Case\ \cap >30) = \frac{(Total Case)*(Total >30)}{Total} = \frac{m_1*n_1}{N} = \frac{3220*2181
}{13465}$$

```{r}
data_table
N = data_table[3,3]
Case_tot    = data_table[1,3]
Ctl_tot     = data_table[2,3]
Over30_tot  = data_table[3,1]
Under30_tot = data_table[3,2]

Expected = c( Case_tot * Over30_tot / N,
              Ctl_tot  * Over30_tot / N,
              Case_tot * Under30_tot / N,
              Ctl_tot  * Under30_tot / N)

ExpectedValues = matrix(Expected,nrow=2,ncol=2)
ExpectedValues
```

We can use the `pchisq` function to get a $p$-value for the cumulative Chi-squared distribution with one degree of freedom, $P(\chi^2_{1,1-\alpha} \ge X^2)$:

```{r}
#data_matrix
chisq = sum( (data_table[1:2,1:2] - ExpectedValues)^2 / ExpectedValues )
chisq
pchisq(chisq, df = 1, lower.tail = F)
```

The R command for the Chi-squared test is `chisq.test`:
```{r}
chisq.test(data_table[1:2,1:2])     # default is correct = TRUE
chisq.test(data_table[1:2,1:2])$p.value

# result is the same regardless of table orientation
chisq.test(t(data_table[1:2,1:2]))$p.value
```

Notice that the manual calculation using `pchisq` does not give exactly the same $p$-value as `chisq.test`. This is because the approximation we made by hand does not correct for the fact that $X^2$ is a continuous function, and thus underestimates the true $p$-values based on the binomial distribution. In contrast, `chisq.test` uses Yatesâ€™ correction by default. 

We can apply Yates' correction manually by subtracting 1/2 from each of the squared differences above, which will give us the same result at `chisq.test`:
```{r}
# Yates' correction for continuity
chisq = sum( (abs(data_table[1:2,1:2] - ExpectedValues) - 0.5)^2 / ExpectedValues )
chisq
pchisq(chisq, df = 1, lower.tail = F)
```


## Binomial proportions

Specifically, we want to know if the **proportion** of women who gave birth at an older age is significantly higher in the group of women WITH breast cancer (call this $\hat{p}_1$) than it is in the group of women WITHOUT breast cancer (call this $\hat{p}_2$). Under the null hypothesis $H_0$, the two proportions should be equal, so their difference should be zero:

$$H_0: \hat{p}_{(Cancer\ \cap Above30)} - \hat{p}_{(Normal \cap Above30)} =\hat{p}_1 - \hat{p}_2 = 0$$

### Proportions

The population mean and variance are then given by: $\mu = p$ and $\sigma^2 = pq/n$, where $q = 1 - p$. We don't know the overall population proportion, but we can estimate it as the the weighted average of the sample proportions:

$$ \hat{p} = \frac{Over30}{Total} = \frac{x_1 + x_2}{n_1 + n_2}$$
where $x_1$ and $x_2$ are the number of women above 30 in each sample (Case = cancer, Control = normal) and $n_1$ and $n_2$ are the total number of women in each sample.

Fortunately, we can use the ``prop.test`` function to calculate if the difference in the proportions is significant using this model. Since we expect the Case group to have a higher proportion of women who first gave birth above the age of 30, our alternative hypothesis is $H_A: \hat{p_1} > \hat{p_2}$, so we will choose a one-tailed test:

```{r}
## two-sample test for equal proportions
# prop.test(x,n,p=NULL,correct=TRUE)
#   look at the help section on how to use this command:
#     x = vector or 1D table with 2 entries giving # of "successes" in each sample,
#         or (SEE BELOW) a 2x2 table or matrix with # of sucesses and failures
#     n = vector giving total number of samples
#     correct = correction for continuous approximation of the binomial (default)

## Data --
# Case group (with cancer):         683/3220 are in "Above30" group
# Control group (without cancer): 1498/10245 are in "Above30" group

## Syntax method 1: using raw data
# "successes" are "Above30", so            x = c( 683, 1498)
# totals in each group (Case, Control) are n = c(3220,10245)

prop.test(c(683,1498),c(3220,10245), alternative="greater") 

## Syntax method 2: using a matrix
# Notes:
#  1) If a matrix is given instead of vector x, then n is ignored.
#  2) A two-dimensional table (or matrix) is expected with 2 COLUMNS giving 
#     the number of "successes" (Above30) and "failures" (Under30), respectively.
#     Thus, the ROWS are the samples being compared (Case, Control).


prop.test(data_table[1:2,1:2], alternative="greater")

# The function returns a data structure with a lot of different pieces of information.
# You can recover the p-value directly from the test result:
prop.test(data_table[1:2,1:2], alternative="greater")$p.value

# NOTE: if table orientation is flipped, we change our frame of reference, 
# so the reported proportions and confidence interval are different ...
prop.test(t(data_table[1:2,1:2]), alternative="greater")

# ... but the p-value is the same!
# This is because the test statistic itself is agnostic to table orientation.
prop.test(t(data_table[1:2,1:2]), alternative="greater")$p.value

# Note that `prop.test` reports the Chi-squared statistic ("X-squared") with df = 1.
# Though the conceptual framework is different, the z-score and X-squared statistics
# are mathematically equivalent for two-way comparisons. 
# (In fact, for a two-sided test, `prop.test` calls `chisq.test` underneath.)
# You can examine the code for this (or any) function using `getAnywhere(prop.test)`.
```

The extremely low $p$-value suggests that there is indeed an association between age of first child birth and breast cancer.

The equations for the distribution of $\hat{p}_1 - \hat{p}_2$ and the $z$-score are explained in Rosner, Section 10.2 and summarized in Eq. 10.3. The default for the `prop.test` is to use Yates' correction for continuity, since the (continuous) normal approximation underestimates the true $p$-values based on the binomial distribution.

As a rule of thumb, **the normal approximation is considered valid whenever $n_1\hat{p}\hat{q} \ge 5$ and $n_2\hat{p}\hat{q} \ge 5$**. When this does _**not**_ hold, **Fisher's Exact Test** is recommended (coming in a later chapter).

The resulting $p$-value of a $\chi^2$ test will be the same as for a _**two-tailed**_ `prop.test`.

We can confirm that the $p$-value returned by `chisq.test` is the same as for a two-tailed `prop.test`:
```{r}
chisq.test(data_table[1:2,1:2])$p.value

# two-sided prop.test
prop.test(c(683,1498),c(3220,10245))$p.value
```

**Note: The $\chi^2$ test is not recommended for cases where expected value in any cell is less than 5**; instead, **Fisher's Exact Test** is recommended.




## Exercise

22,071 physicians served as subjects to study effect of aspirin on incidence of heart attacks. There are two groups: those who took aspirin regularly for 5 years and others (11,043 of the 22,071) received placebo instead of aspirin. 189 of the group that took placebo suffered a heart attack and 104 of those who took aspirin suffered a heart attack. Does aspirin have a significant association with physicians suffering from a heart attack ? 

1. Create a contingency table for these data.

```{r}

```

2. Calculate the expected values.

```{r}

```


3. Calculate the $X^2$ value using your observed and expected counts.

```{r}



```


4. What is the $p$-value of the test statistic?

```{r}


```
