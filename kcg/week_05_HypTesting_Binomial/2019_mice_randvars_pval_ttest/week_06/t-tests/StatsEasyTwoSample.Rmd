---
title: "StatsEasy"
author: "Manpreet S. Katari"
output: 
  html_document: default
  github_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The following examples are taken from __Data and Notes taken from Sasha and Wilson (Morgan & Claypool)__ chapters 1 and 2. [DOWNLOAD](http://www.morganclaypool.com/doi/pdf/10.2200/S00295ED1V01Y201009MAS008)


## Placebo vs Drug

People were given a *Drug* or a *Placebo* and effect was measured for both. We want to know if the *Drug* effect is significantly greater.

``Placebo = c(54, 51, 58, 44, 55, 52, 42, 47, 58, 46)``

``Drug = c(54, 73, 53, 70, 73, 68, 52, 65, 65, 70)``


```{r}

Placebo = c(54, 51, 58, 44, 55, 52, 42, 47, 58, 46)
Drug = c(54, 73, 53, 70, 73, 68, 52, 65, 65, 70)

Placebo_mean = mean(Placebo)
Placebo_mean
```

```{r}
Drug_mean = mean(Drug)
Drug_mean
```

### Hypothesis testing
The Drug_mean is greater than Placebo_mean, but how do we know it is significant? What are the chances that these values are completely random?

If the data from the two groups actually come from the same population, then the difference between them is due to random sampling, so the labels shouldn't matter. Let's shuffle the labels and take the distance between the means of the randomly shuffled sets to see how different the randomized means are. Let's do this 10,000 times and count the number of times the result exceeds or equals the difference between the means of the original groups.

```{r}

# We first combine the Placebo and Drug values
dataValues = c(Placebo, Drug)

# Then we create a character vector that specifies which value is from
#which group
dataLabels = c(rep("P",times=length(Placebo)),
               rep("D",times=length(Drug)))

# We can use the tapply() function to calculate the mean of each group.
# It takes the values, then separates them based on the labels
# and then takes the respective means of the different groups.

real_means = tapply(dataValues, dataLabels, mean)
real_means

# Note that tapply takes your data labels and puts them in ALPHABETICAL ORDER
```

```{r}

# There is also a function that simply takes the difference of two values

real_diff = diff(real_means)
```

Now let's shuffle the labels randomly using the ``sample()`` function and recalculate their difference.

```{r}

num_shuffles = 10000

# avg_diff is where we are going to save all the results.
avg_diff = numeric()

# repeat the following 10,000 times:
for ( i in 1:num_shuffles) {

  # sample from the data labels in a random order
  shuffleLabels = sample(dataLabels,
                         length(dataLabels),
                         replace=F)
  
  # now get the means of the two randomized groups
  avg_diff[i] = diff(tapply(dataValues,
                            shuffleLabels, 
                            mean))
}

# now look at the distribution of the difference between the means of the two randomized groups
hist(avg_diff)
```

If we look at the histogram of the differences between groups generated by shuffling our labels, we see that most of them are close to 0. So how many times would we expect the difference to be greater than or equal to the difference between the real groups, `real_diff`?

```{r}
length(which(real_diff >= avg_diff))/num_shuffles
```

This means that it is extremely unlikely that the labels don't matter.

The next question is: even though the results show that the difference is significant, does that mean it is important?


### Confidence Intervals

Instead of calculating a p-value, maybe we should think of it in terms of how *sure* we are (usually 90 to 95%) that **your** original difference (the one you observed in your experiment) is within a *range of values*.


#### Bootstrapping

Bootstrapping is a way to sample from the original dataset and allow for a specific value to be repeated. This generates a possible combination of values that you could have obtained. 

So we will select same number of Placebo values from the Placebo set, however allow for a given Placebo value to be selected more than once. We will do the same for the Drug values. We will again record the difference between the two groups and sort them. The values at the bottom 5th percent and the top 5th percent (i.e. 95th percentile) will define a range. This range will give us a 90% confidence that the real value is somewhere in this range.

More precisely, the **90% Confidence Interval** tells us that 90 times out of 100, if we take the same size samples from the Placebo and Drug cohorts, the true difference of the means will fall within this range.

```{r}

num_shuffles = 10000

avg_diff = numeric()

for ( i in 1:num_shuffles) {
  PlaceboSamples = sample(Placebo, length(Placebo), replace=T)
  DrugSamples = sample(Drug, length(Drug), replace=T)
  avg_diff[i] = mean(DrugSamples) - mean(PlaceboSamples)
}

hist(avg_diff)
```

```{r}
# the bottom of the 95% Confidence Interval
sort(avg_diff)[250]
```

```{r}
# the top of the 95% Confidence Interval
sort(avg_diff)[9750]
```

Now we can report that the 95% CI is the range between these two numbers.


### Using the t-distribution - Distribution of sample means (Standard Error)

If we randomly sample 5 numbers from the data and calculate its mean every time, then we will get a distribution known as the **t-distribution**. The *standard deviation of the t-distribution* is called the **standard error**, or the **standard error of the mean**. The sampling below shows that the values we calculate are close to what we get if we assume the data is normally distributed.

```{r}
D_sample=numeric()
n=5
for (i in 1:10000) {
  
  # recall that dataValues = c(Placebo, Drug) so we are just taking the mean of all the data
  D_sample[i] = mean(dataValues[sample(1:length(dataValues), n, replace = T)])
}

D_sample_sd = sd(D_sample)
D_sample_sd
```

Compare the value to the formula for the standard error:

```{r}
sd(dataValues)/sqrt(n)
```

We can use the distribution of the standard error to determine our confidence interval.

```{r}
D_sample_mean = mean(D_sample)
D_sample_mean
hist(D_sample)
```


#### Different **n**

Let's see if the number of samples that we take from the population will have an effect on the standard error.

```{r}
D_sample=numeric()
n=10
for (i in 1:10000) {
  D_sample[i] = mean(dataValues[sample(1:length(dataValues), n, replace = T)])
}

D_sample_sd = sd(D_sample)
D_sample_sd
```

```{r}
hist(D_sample)
```

Notice that by using samples that are twice as large, we decreased the SD, but not by half!


#### 95% confidence interval of Placebo

Once we have the standard error we can say that we expect the mean of sample to be within this range 95% of the time. We can use the family of R functions for the t-distribution to calculate the 95% CI.

```{r}
P_se = sd(Placebo)/sqrt(length(Placebo))

# Remember that we have n-1 degress of freedom ...
lower_limit = mean(Placebo) + qt(p = 0.025,df = length(Placebo)-1) * P_se
upper_limit = mean(Placebo) - qt(p = 0.975,df = length(Placebo)-1, lower.tail = F) * P_se
lower_limit
upper_limit
```

The t-test function gives us the same information:

```{r}
d_mean = mean(dataValues)
d_mean
t.test(Placebo, mu=d_mean)

```

### Two samples

When we look at two samples we are interested in the difference between the means of the samples. 

```{r}
# compute the difference of the means for 10,000 independent samples of size 5 
# taken from each of the Placebo and Drug populations separately

Diff_sample=numeric()
n=5
for (i in 1:10000) {
  # sample n random integers, with replacement, from 1 up to the number of items in P and D
  # use these as indeces to take 5 random samples from each of the P and D vectors
  # then take the mean of each and compute the difference betwen means
  Diff_sample[i] = mean(Placebo[sample(1:length(Placebo), n, replace = T)]) -
                   mean(Drug[sample(1:length(Drug), n, replace = T)])
  
}

hist(Diff_sample)

```

The standard error of a two-sample dataset is slightly different from what you expect. It is the square root of the sums of the standard errors.

```{r}

# sd of the difference between the means
se_diff = ( sqrt((sd(Placebo)**2)/length(Placebo) + (sd(Drug)**2)/length(Drug)) )

# difference of the means
mean(Drug) - mean(Placebo)

tstat = ( mean(Drug) - mean(Placebo) ) / se_diff
tstat
```

The t-test function assumes that the data is normal distribution and in this case, the variance is equal. 

```{r}
t.test(Placebo, Drug, var.equal = T)

```


## Bootstrapping and Data reliability

Bootstrapping can also help determine if the data is reliable - for example does it have outliers. In a probabilistic world, a significantly large outlier can have a strong effect on parameters such as  the mean and standard deviation.

###  Income
Let's pretend we have incomes from several people and our goal is to calculate the average income. However, since one of our values is a billionaire, the average is skewed high. We can bootstrap the incomes to calculate the 90% confidence as described in the previous section. If our 90% confidence interval is too large, then the mean is not very useful.


```{r}

incomes = c(200, 69, 141, 45, 154, 169, 142, 198,
            178, 197, 1000000, 166, 188, 178, 129,
            87, 151, 101, 187, 154)

mean(incomes)
```

```{r}
average_income = numeric()
for (i in 1:10000) {
  average_income[i] = mean(sample(incomes,
                                  length(incomes),
                                  replace=T))
}
```

```{r}
sort(average_income)[500]
```

```{r}
sort(average_income)[9500]
```

By bootstrapping the values we are able to determine how reliable is the mean of the population.
