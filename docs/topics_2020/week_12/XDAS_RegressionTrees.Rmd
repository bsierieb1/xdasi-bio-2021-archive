---
title: "Linear regression exercise"
subtitle: "XDAS-2020 11/16/2020"
author: "Manny Katari"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Correlation vs Regression

What is the difference between **Correlation** and **Regression**? 

Correlation is looking to describe the variation of the points from the best fit line. Regression is more about using the line to make predictions of the response variable, $Y$, when new values of explanatory variables, $X$, are provided. 

# Least Squares

The equation of the line is $Y=a + bX$ where $a$ is the $Y$ intercept and $b$ is the slope coefficient of the line. Using the Lease Squares method we can calculate the slope using the following formula:

$$ b = \frac{\sum_i (X_i - \bar{X}){(Y_i - \bar{Y})}}{\sum_i(X_i - \bar{X})^2} $$
Once the slope is calculated we can plug in the values to calculate the Y-intercept. For the values of $X$ and $Y$ we will simply use \bar{X} and \bar{Y} because the line must go through this point. 
    
$$ a = \bar{Y} - b \bar{X}$$

# The trees dataset

A dataset called ``trees`` contains information about ``Girth`` ( diameter of the tree ), ``Height`` and ``Volume`` of 31 cherry trees. Let's determine which variable is a better predictor of Volume.

## Calculate slope and y-intercept when predicting volume using height.

```{r}
# load dataset
head(trees)
(volume_mean = mean(trees$Volume))
(height_mean = mean(trees$Height))
(b = sum((trees$Volume - volume_mean)*(trees$Height - height_mean)) / sum((trees$Height - height_mean)**2))
(a = volume_mean - b*height_mean)
```

## Create linear model volume ~ height
```{r}
# create linear model -- the two expressions below are equivalent
# tree_lm = lm(trees$Volume ~ trees$Height)
tree_lm = lm(Volume ~ Height, data = trees)
summary(tree_lm)
```



## Inspect plots and determine if lm is of good quality

- Is there a transformation that could help the model?
- Are there any outliers?
- Is the variance/residual equal around the model?

```{r}
# plot height by volume and the regression line
library(ggplot2)

ggplot(trees, aes(x=Height, y=Volume))+
    geom_point()+
    stat_smooth(method="lm",
             col="red")

```


## LM plots

Once you create an linear model, you can inspect the model by using the standard plots that are often used in assessing the linear model. 

Here we will use a ggplot2 companion package called ggfortify which takes the normal baseR graphics and converts them into `ggplot` graphs.



```{r}
# look at various plots for the model
library(ggfortify)
tree_lm_plots = autoplot(tree_lm)
```

### Residuals vs Fitted

The first plot looks at the Residuals vs the Fitted. The x-axis are the fitted values and the y-axis are the residuals. Remember residual is the distance between the data point and the predicted point for that x-value. Ideally the points should be as close to the dotted line ( where residual is 0 ). This graph also demonstrates the shape of the relationship. If it is a linear relationship the fitted line should be straigth, but in this case it looks like its polynomial.

Also note that three of the points are numbered, this is the position of the point in the dataframe and it brings your attention to them because they are outliers.

```{r}
tree_lm_plots[1]
```

### Normal QQ plot

This plot is quantile - quantile plot which shows you whether the residuals are normally distributed. Again the best case scenario is that the points fall on the dashed line. Here again it looks like the data is nor normally distributed and may need to be transformed.

```{r}
tree_lm_plots[2]
```

### Scale location plot

This plot is used to determine if the variance is equal thoughout the data. Homoscedasticity, or equal variance, is an assumption that is made when performing certain statistical tests. The more horizontal the data, the more equal the variance is across the data. In this example the variance increases as the predicted value increases.

```{r}
tree_lm_plots[3]
```

### Residuals vs Leverage

Cook's distance is calculated by removing a specific data point and checking how much the model has changed. So the greater the Cook's distance the more influential that point is to the model, and in this case that is not good. When creating linear models, the outliers can be very disruptive to the actual model of the data and so this is a good way to determine if there are indeed outliers. 

In the graph below, the threshold for Cook's distance is presented by a dashed red line, so in this case, even though it looks like we have an outlier, it doesn't disrupt the model too much.

```{r}
tree_lm_plots[4]
```

# Residual

Residual is the difference between the fitted value and that of the point that was used to create the model. The fitted values are often referred to as $\hat{Y}$ and can be accessed from the result of the `lm` function.

```{r}
head(tree_lm$fitted.values)
```

Similarly we can also retrieve the residuals from the result `lm` object:

```{r}
head(tree_lm$residuals)

```

# Standard error of the slope

The $SE_{slope}$ is defined as 

$$SE_{slope} = \sqrt{\frac{MD_{residual}}{\sum_i (X_i - \bar{X})^2}} = \sqrt{\frac{\frac{\sum_{i} (Y_i - \hat{Y})^2}{n-2}}{\sum_i (X_i - \bar{X})^2}}$$
Calculating it using the values provided we get

```{r}
SE_slope = sqrt((sum((trees$Volume - tree_lm$fitted.values)**2) /
                (length(tree_lm$fitted.values)-2)) /
    (sum((trees$Height - height_mean)**2))) 

```

# T-test of the slope

The t-statistic for the slope is also very similar to the t-statistic we have seen earlier. We simply take the slope and divide it by the expected slope, in this case it would be 0 and divide it by the standard error. We can then use the `pt` function to calculate the p-value:

```{r}
slope_t = b/SE_slope
pt(slope_t, 29, lower.tail=F)*2

```

# R-squared value

The $R^2$ value is similar to that of ANOVA, where we take the ratio of the $SS_{regression}$ and $SS_{total}$:

$$R^2 = \frac{SS_{regression}}{SS_{total}} = \frac{ \sum_{i} (\hat{Y} - \bar{Y})^2 }{ \sum_{i} (Y_i - \bar{Y})^2 }$$

The $SS_{total}$ is simply the difference of the $Y$ from $\bar{Y}$ and the regression is the difference between $\bar{Y}$ and $\hat{Y}$.

```{r}
r_2 = sum((tree_lm$fitted.values - volume_mean)**2)/
   sum((trees$Volume - volume_mean)**2)
r_2
```

## Adjusted R-squared

The adjusted $R^2$ adjusts for the the number of variables being used to model the data. It penalizes you for adding variable that are not playing any role in explaining the response variable.

$$R_{adj}^2 = \bigg(\frac{SSE}{SST}\bigg)  \bigg(\frac{n-1}{n-p}\bigg)$$
```{r}
1- ((1-r_2) * ((31-1)/(31-2)))

```

# Plot standard error

Pick a range of values (60:90) and then use the best linear model for Girth to predict the Volume, and the lower and upper intervals.  

```{r}
range_predict = seq(from=60, to=90, by=1)

tree_lm_predict = predict.lm(tree_lm, 
                                 newdata = data.frame(Height=range_predict),
                                 interval="confidence")

```

- Use ggplot2 to plot the predicted values and the lower and upper intervals for the values 0:25. 

```{r}
library(ggplot2)
library(reshape2)

tree_predict.melt = melt(tree_lm_predict)

ggplot(data = tree_predict.melt, 
       aes(x=Var1, y=value, group=Var2)) + geom_line(aes(color=Var2))

```

# Transform data

```{r}

plot(lm(log(trees$Volume) ~ trees$Height))

```
```{r}
summary(lm(log(trees$Volume) ~ trees$Height))

```

# In class exercise

## Do the same for Girth. 

    - Are the values normally distributed?
      - Create histograms and use shapiro.test
    - Create a linear model
      - What is the slope? is it significant?
      - What is the adjust $R_2$ 
    - Summarize each of the 4 plots
      - Are there any outliers?
      - Is the variance/residual equal around the model?

    


