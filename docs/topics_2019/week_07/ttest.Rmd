---
title: "T-test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Simple case
Here will discuss a simple case study where the a drug has been provided to 10 random patients (or test subjects) and, as a control, a placebo pill was given to 10 other random patients. Each condition collected measurements and the question is:

## Is there a significant difference between the control subjects and the those who were given the drug?

### The Population

Let's first look at all values as a population. We will combine the values and draw a simple histogram.

```{r}

Placebo = c(54,51,58,44,55,52,42,47,58,46)
Drug = c(54,73,53,70,73,68,52,65,65,60)

Data = c(Placebo, Drug)
Data_sd = sd(Data)
hist(Data)

```

### Bootstrapping

Bootstrapping is when we sample our dataset with replacement. The idea is that we use the same data values to approximate what the population may look like. Let's simulate randomly obtaining 10 measurements from the sample. If we do this 10,000 times, we will see that the distribution of the samples of the population gives a normal distribution. In fact, the standard deviation of this distribution is called the Standard Error (SE). 

```{r}
P_sample=numeric()
n=10
for (i in 1:10000) {
  P_sample[i] = mean(Placebo[sample(1:length(Placebo), n, replace=T)])
}

hist(P_sample)
sd(P_sample)

```

Notice that the SE changes when we select a different number of samples (5 instead of 10). You will notice that the SE decreases as the number of samples (n) increases. This relationship can be described as:

$$SE_{\bar{x}} = {\frac{\sigma}{\sqrt{n}}}$$

```{r}
P_sample=numeric()
n=5
for (i in 1:10000) {
  P_sample[i] = mean(Placebo[sample(1:length(Placebo), n, replace=T)])
}

hist(P_sample)
sd(P_sample)


```

### Confidence Interval

The standard error is very helpful because it gives us an idea of how close we are to the actual mean. We can use the SE to help define Confidence Intervals (CI) of what the the actual population mean is.

Let's take our Placebo sample and try to calculate our 95% confidence interval for the mean of the population. In this case, let's pretend we don't know the population and Placebo is just a sample from some unknown population. Since we don't have the $\sigma$ we will estimate it by calculating the $sd$ of the sample. Note that this only works if the population is truly normally distributed. This estimated distribution will give us a t-distribution. It is similar to a standard normal distribution but with heavier tails. 

We can use the qt() function to identify the t-score where df is one less than the number of samples and the area under the curve is 97.5%.

We can add and subtract this amount from the mean to get our 95% CI.

```{r}
Pse=sd(Placebo)/sqrt(length(Placebo))
Pse

t_0.975 = qt(p = .975, df = length(Placebo)-1)
Pse * t_0.975
Pmean = mean(Placebo)
Pmean - Pse * t_0.975
Pmean + Pse * t_0.975
```

We can check our answer by using the t.test() function where we can provide the $\bar{x}$ since we don't know the $\mu$ ( population average )

```{r}
t.test(Placebo, mu=mean(Placebo))
```

### Known $\mu$ - One group t-test

If we do know $\mu$, then we test to see how likely our sample is from this population. We first calculte the t-stat by seeing how many SE the Placebo mean is from the population mean. Then we can look to see the area of under the curve for that stat. Since we are considering both extremes, we should multiply the p-value by 2.

```{r}
(Pmean - mean(Data))/Pse

# Because we want to look at both extreme values (lower 2.5% and higher 2.5%),
# and we know the distribution is summterical, simply multiply p-value by 2.
pt((Pmean - mean(Data))/Pse, df = 9)*2

```

Our null hypothesis in this case is that there is no difference between Placebo mean and Population mean. If this was true, our T-statistics would be low. But our T-statistic was high giving us a p-value of less than 0.05 ( the standard cutoff for considering something significant ) therefore we consider the alternate hypothesis which, in this case, is the difference between the Placebo and the Population is significantly higher or lower.

```{r}
t.test(Placebo, mu=mean(Data))

```

### Difference of means is also normally distributed

Now let's start to consider that Placebo and Drug are two separate populations. If we randomly sampled from the two different populations, we can determine ( with 95% confidence ) what is the actual difference in the means of the populations.

Below I will sample 5 values form both populations and simply take their difference.

```{r}

Diff_sample = numeric()
n=5
for (i in 1:10000) {
  Diff_sample[i] = mean(Placebo[sample(1:length(Placebo),n,replace=T)])-
    mean(Drug[sample(1:length(Drug), n, replace = T)])
}

hist(Diff_sample)

```

Turns out the SE of the difference is the square root of the sum of the SE of each sample:

$$\sigma_{\mu_{1}-\mu_{2}}={\sqrt{\frac{\sigma_{1}^2}{n_1}+{\frac{\sigma_{2}^2}{n_2}}}}$$

```{r}
se_diff = ( sqrt (
  ((sd(Placebo)**2)/length(Placebo)) + 
    ((sd(Drug)**2)/length(Drug))))

se_diff

```

We can caculate the 95% confidence interval by using the t-distribution only the degree of freedom in this case is the sum of the total values minus the number of samples ( 20 - 2 ).

```{r}

Diff_mean = mean(Placebo) - mean(Drug)

Diff_mean + qt(p=0.025, df=18)*se_diff
Diff_mean - qt(p=0.025, df=18)*se_diff
```

Notice that the confidence interval does not span 0, which means it is extremely unlikely that the true difference between the Drug and Placebo populations is 0.

Let's check is this is true.

We can look at the T-statistics ( which is the equivalent to the Z-score if it was normally distributed ) we simply divide the difference of the sample by the size of SE.

```{r}

tstat = Diff_mean / se_diff
tstat
```

Now we can calculate the p-value that our null hypothesis is true. The null hypothesis is that the difference between the two populations is 0. We know this because if we were to plot a distribution from random variables with the same mean, we would expect to get a difference of 0.

```{r}
#using our calculations done by hand
2*pt(q = tstat, 
     df = (length(Placebo)+length(Drug)-2), 
     lower.tail = T)

#confirmation using the t.test() function.
t.test(Placebo, Drug, var.equal = T)
```

### What if the data is taken from the same individuals? Data is then paired.

Paired t-test is essentially the one-group t-test using the difference between the values.

Assuming the data from the two vectors are in the same order as the corresponding individuals, we can simply subtract them. Our null hypothesis would again be that the difference of the two groups is zero.

```{r}
DrugDiff = Placebo-Drug
```


Let's calculate the ``t-statistic`` can be described as

$$ t* = \frac{\bar{X}_{D} - D_0}{\frac{S_{D}}{\sqrt{n}}} $$

where $\bar{X_D}$ is the mean of the differences and $D_0$ is what we are testing, which in this case is $0$ and $S_D$ is the standard deviation of all differences.

```{r}
t_stat = mean(DrugDiff)/(sd(DrugDiff)/sqrt(length(DrugDiff)))

2*pt(t_stat, df=9, lower.tail = T)
```

```{r}
t.test(DrugDiff, mu=0)
```

We can also confirm that one group t-test is same as t-test with the paired option

```{r}
t.test(Placebo, Drug, paired = T, var.equal = T)

```

### What if the data is not normally distributed?

Instead of looking at the actual values, let's look at the ranks of the values. Since we are looking at ranks, our null hypothesis is that the sum of the ranks from one group is the same as the other groups. This is the **Wilcoxon signed-rank test** which is a non-parametric equivalent to a paired t-test. 

The steps are:

+ Calculate the differences between the pairs

+ _n_ is the number of nonzero differences

+ Rank the *absolute* values of n differences. Assign the average if there is a tie

+ Re assign to a group based on whether it was positive or negative change.

+ The sum of the ranks that is lower ( positive or negative ) is the statistic.


```{r}
Data

DrugDiff = Placebo - Drug
print(DrugDiff)

DrugDiff = DrugDiff[which(DrugDiff != 0)]
print(DrugDiff)

DrugDiffRank = rank(abs(DrugDiff))
print(DrugDiffRank)

t_neg = DrugDiffRank[DrugDiff < 0]
t_neg

t_pos = DrugDiffRank[DrugDiff > 0]
t_pos

t_min = min(t_pos,t_neg)
t_min

n_diff = length(DrugDiff)
n_diff
```


The population mean for the statistic is 

$$ \mu_{v} = \frac{n(n+1)}{4}$$

Since for us the n was 9 ( we had one zero diff ) 

$$ \mu_{v} = \frac{9(10)}{4} = 22.5$$
```{r}
mu_diff = n_diff*(n_diff+1)/4
mu_diff
```

The population is a bit complicated for ties so let's just focus on when there are no ties.

$$ \sigma_{V}^2 = \frac{n(n+1)(2n+1)}{24} $$

```{r}
var_diff = n_diff*(n_diff + 1)*(2*n_diff + 1)/24
var_diff
```

To get the standard deviation we simply take the square root.

```{r}
sd_diff = sqrt(var_diff)
sd_diff
```

When there are plenty of samples (usually > 25), a normal approximation is used to estimate the p-value. Let's just assume we do and caclulate the *z-statistic* and determine our p-value

```{r}
z_diff = (t_min - mu_diff)/sd_diff
z_diff

2*pnorm(z_diff, lower.tail = T)

```

Let's confirm this

```{r}
wilcox.test(Placebo, Drug, paired = T)

```

### What is our data is not normal but also not paired

We can use the wilcox test but instead of separating them by positive and negative, we will separate them by their group

The steps are as follows:

+ Combine the values and rank them together.

+ Rank them (smallest to largest where smallest gets a 1)

+ If there is a tie, provide the average as the rank for all.

+ $T_{1}$ is the sum of the rank group one and $T_{2}$ is the sum of the ranks 
in group two.

+ Calculate W using the formulas below:

$$ W_{1} = T_{1} - \frac{n_{1}(n_{1}+1)}{2} $$
$$ W_{2} = T_{2} - \frac{n_{2}(n_{2}+1)}{2} $$
where $n_{1}$ and $n_{2}$ are the size of the two groups.

```{r}
Data
DataRank = rank(abs(Data))
DataRank

DataRankSums = tapply(DataRank,rep(c("P","D"),each=10),sum)
DataRankSums

n_placebo = length(Placebo)
n_drug = length(Drug)

t_P  = DataRankSums["P"]
t_D  = DataRankSums["D"]

w_P = t_P - (n_placebo*(n_placebo + 1)/2)
w_P
w_D = t_D - (n_drug*(n_drug + 1)/2)
w_D
```

The population variance ( again, ignoring the term for ties for now ) is 

$$ \sigma_{W}^2 = \frac{n_{1}n_{2}}{12} (n_1 + n_2 + 1)  $$
```{r}

sd_w_squared = (n_placebo*n_drug/12) * (n_placebo + n_drug + 1)
sd_w = sqrt(sd_w_squared)
```

And the population mean is :

$$ \mu_W = \frac{n_1n_2}{2}$$

```{r}
mu_w = n_placebo * n_drug / 2
mu_w
```

Again, with high number of samples we can approximate to normal distribution, so let's just calculate the z score and get our p-values.

```{r}
z_w = (w_D-mu_w)/sd_w
z_w

2*pnorm(z_w, lower.tail = F)
2*pnorm(w_D, mean=mu_w, sd=sd_w, lower.tail = F)
```

Confirmation that our calculation is correct

```{r}
wilcox.test(Placebo, Drug)

```

### Exercise

#### Perform a bootstrap analysis of non-parametric methods

+ Perform a permutation analysis demonstrating the mean of the $W$ statistic in two group wilcoxon test is correct.

+ Perform a bootstrap analysis to show the standard deviation of the $W$ statistic in the two group wilcoxon test is correct.

