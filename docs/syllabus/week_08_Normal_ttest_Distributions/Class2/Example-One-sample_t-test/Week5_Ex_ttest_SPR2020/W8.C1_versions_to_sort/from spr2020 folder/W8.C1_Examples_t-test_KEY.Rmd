---
title: Parametric and non-parametric tests for one-sample, paired, and two-sample
  tests
output:
  pdf_document:
    toc: yes
    toc_depth: '4'
  html_document:
    toc: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
```


## Review

Today we will review concepts and practice different kinds of significance tests that are appropriate for different experimental scenarios. First, review from last time:

+ Random samples
+ Standard normal distribution
  - What is a $z$-score?
  - What are the quantiles?
  - How do you capture height vs. area? 
+ Sampling distribution of the sample mean
+ Standard error
+ Confidence intervals
+ What is a $z$-score?
+ $t$-statistics vs. $z$-statistics : when is each appropriate?
+ P values : one- vs. two-sided
+ What does qt(0.975) represent?


## One-sample, paired-sample, and two-sample $t$-tests

### The data

Here will discuss a simple case study where a drug has been provided to 10 random patients (or test subjects) and compared to the effect of a placebo pill given other random patients. Measurements were collected for each condition to answer the question:

#### Is there a significant difference between the control subjects and the those who were given the drug?

```{r}
# the population
Placebo_pop = read.csv("placebo_pop", header = FALSE)
Placebo_pop = Placebo_pop[,1]

# sample data
Placebo = c(54,51,58,44,55,52,42,47,58,46)
Drug = c(54,73,53,70,73,68,52,65,65,60)
```


### Look at the placebo population distribution

```{r}
## visualize the results
hist(Placebo_pop, breaks=20, freq=FALSE)
xfit = seq(40,60,length=100)
yfit = dnorm(xfit,mean=mean(Placebo_pop),sd=sd(Placebo_pop))
lines(xfit,yfit,col="blue",lwd = 2)

```

### Confidence Interval

The standard error is very helpful because it gives us an idea of how close our data are to the actual mean. We can use the SE to help define **Confidence Intervals (CI)** of what the the actual population mean is.

Here, we assume that we have random samples and that the measurements are normally distributed in the population. The estimated distribution of the sample means will then follow a $t$-distribution, which is similar to a standard normal distribution but with heavier tails. As the sample size increases, the tails of the $t$-distribution become smaller and it resembles closely the $Z$-distribution.

Let's take our Drug sample and try to calculate our 95% CI for the mean of the population, assuming that the drug has no effect.

+ Estimate the mean and SE for the Drug sample.
+ Use the `qt()` *quantile function* to identify the critical $t$-score for the the 95% CI with the appropriate $df$.
+ Use the estimated mean and $t_{crit}$ to get our 95% CI.

```{r}
Dse=sd(Drug)/sqrt(length(Drug))
## [1] 2.564934
t_0.975 = qt(p = .975, df = length(Drug)-1) # 2.26 = z-score for 97.5% area
## [1] 2.262157
Dse * t_0.975
## [1] 5.802285
# now can calculate 95% CI
Dmean = mean(Drug)
c(Dmean - Dse * t_0.975, Dmean + Dse * t_0.975) ## [1] 57.49772 69.10228
# use sample SD to approximate population SD
# now can get an estimate how how likely it is that we have captured the true pop mean
```

We can check our answer by using the `t.test()` function. Do this for both the Drug and the Placebo samples. What do you conclude?

```{r}
t.test(Placebo, mu=mean(Placebo_pop)) ##
## One Sample t-test
##
## data:  Placebo
## t = 0.0014947, df = 9, p-value = 0.9988
## alternative hypothesis: true mean is not equal to 50.6973 ## 95 percent confidence interval:
## 46.6107 54.7893
## sample estimates:
## mean of x
## 50.7

t.test(Drug, mu=mean(Placebo_pop)) ##
## One Sample t-test
##
## data: Drug
## t = 4.9135, df = 9, p-value = 0.0008321
## alternative hypothesis: true mean is not equal to 50.6973
## 95 percent confidence interval: ## 57.49772 69.10228
## sample estimates:
## mean of x
## 63.3

```


### One-sample $t$-test using an estimate for the population mean

Now let's pretend we don't know the population, so that Placebo is just a sample from some unknown population. Since we don't know the true $\sigma$, we will estimate it.

First look at the mean and SD of the Placebo population and the Placebo sample.

```{r}
 
## check the parameters from the true population
paste("Pop. mean = ",round(mean(Placebo_pop),3),
      " ; Pop. SD = ",round(sd(Placebo_pop),4))

## check the parameters from the estimated population
paste("Pop. mean (est) = ",round(mean(Placebo),3),
      " ; Pop. SD (set) = ",round(sd(Placebo),4))

```

Do these look pretty different? Why?

#### One-sample t-test with simulated population data

In order to perform a one-sample $t$-test, we technically need to have some data from a control population for comparison, so that we can test our samples against the true population mean. Here, however, we do not have the population data, so what do we do? We can actually reverse engineer a control sample population, using our best estimates of the population parameters using the control sample we do have.

Recall from Chapter 11 that our best estimate for the true population mean $\mu$ is the sample mean $\bar{Y}$. Our best estimate for the true population variance (Section 11.5) is:

$$ \sigma^2 = \frac{(n-1)s^2}{\chi^2} $$

That looks pretty simple, but what do we use for Chi-squared? If we were computing a 95% CI for the population SD, we would use $\chi_{\alpha=0.025}$ and $\chi_{\alpha=0.975}$. Instead, here we want a representative value from the sampling distribution of $\chi^2$, so we will use the 50th quantile (the midway point in the density distribution).

Using this information, we can sample from a normal distribution with our estimated parameters and pretend that this is our control population data. It will probably be closer overall to the Placebo sample than the original population, since we have to use the sample statistics to estimate the population parameters, but that's ok for now.


```{r}
## estimate the population parameters

# mean
pop_mean_est = mean(Placebo)

# s.d. -- use the sample variance and the Chi-square distribution
# sigma^2 = df * s^2 / chisq_est(0.5,9)
# a) compute chi-squared estimate
chisq_est = qchisq(0.5, length(Placebo)-1)

# b) compute pop sd estimate
pop_sd_est = sqrt( (length(Placebo)-1)*sd(Placebo) / chisq_est )

# check the estimated parameters
paste("Chi-square = ",round(chisq_est,4),
      " ; Pop. SD est. = ",round(pop_sd_est,4))

paste("Population mean estimate = ",round(pop_mean_est,3))

## simulate a population using the estimated population mean and sd
pop = rnorm(1000,mean=pop_mean_est,sd=pop_sd_est)

## visualize the results
hist(pop, breaks=20, freq=FALSE)
xfit = seq(40,60,length=100)
yfit = dnorm(xfit,mean=pop_mean_est,sd=pop_sd_est)
lines(xfit,yfit,col="blue",lwd = 2)
```

What's the difference between the true population parameters and the simulated population parameters?

```{r}

## check the parameters from the true population
paste("Pop. mean = ",round(mean(Placebo_pop),3),
      " ; Pop. SD = ",round(sd(Placebo_pop),4))

## check the parameters from the simulated population
paste("Pop. mean (simulated) = ",round(mean(pop),3),
      " ; Pop. SD (simulated) = ",round(sd(pop),4))

```


Calculate the 95% CI for the true Placebo mean using the Placebo sample.

```{r}

```

Let's test our estimates for the Placebo population mean using a $t$-test. We have to provide $\bar{x}$ as the estimate for the true population mean since we are pretending that we don't know $\mu$.

```{r}

```

How do these results differ from the previous test using the true population mean?


### Confidence Intervals for the Population Mean

The standard error is very helpful because it gives us an idea of how close our data are to the actual mean. We can use the SE to help define **Confidence Intervals (CI)** for the the actual population means from which the samples were drawn.

Here, we assume that we have random samples and that the measurements are normally distributed in the population. The estimated distribution of the sample means will then follow a $t$-distribution, which is similar to a standard normal distribution but with heavier tails. As the sample size increases, the tails of the $t$-distribution become smaller and it resembles closely the $Z$-distribution.

Let's calculate the 95% CI for the mean of each population that the samples are drawn from.

+ Estimate the mean and SE for the Placebo and Drug samples.
+ Use the `qt()` *quantile function* to identify the critical $t$-score for the the 95% CI with the appropriate $df$.
+ Use the estimated means, SEs, and $t_{crit}$ to get the 95% CI's for the two samples.

```{r}
## t_critical: # 2.26 = t-score for 97.5% area
# Placebo and Drug are the same length so this works for both
t_0.975 = qt(p = .975, df = length(Placebo)-1)
paste0("t_crit = ",round(t_0.975,4))  # 2.26 = z-score for 97.5% area

## placebo
Pmean = mean(Placebo)
Pse=sd(Placebo)/sqrt(length(Placebo))

# 95% CI
c(Pmean - Pse * t_0.975, Pmean + Pse * t_0.975)

## drug
Dmean = mean(Drug)
Dse=sd(Drug)/sqrt(length(Drug))

# 95% CI
c(Dmean - Dse * t_0.975, Dmean + Dse * t_0.975)
```

We can check our answers by extracting the CI estimates from the `t.test(...)` function. You can inspect the `t.test` object using `str()`; it's a list containing a bunch of information about the results of the $t$-test. The precise expression to get the CI is `t.test()$confint[1:2]`. 

Do this for both the Drug and the Placebo samples. Are these the same as what you calculated above?

```{r}
t.test(Placebo, mu=mean(pop))$conf.int[1:2]
t.test(Drug, mu=mean(pop))$conf.int[1:2]
```

Using the rules of thumb given in your textbook, can you conclude by eye that the samples come from different populations? Why or why not?

<!-- YOUR ANSWER HERE -->


### One-sample $t$-test

Run a $t$-test for each sample against:

+ the mean of the simulated control population, and
+ the population mean estimate from just the Placebo sample.

```{r}
# Placebo sample
t.test(Placebo, mu=mean(Placebo_pop))
t.test(Placebo, mu=mean(Placebo))

# Drug sample
t.test(Drug, mu=mean(Placebo_pop))
t.test(Drug, mu=mean(Placebo))
```


How similar are the results between the tests using the "true" (simulated) population mean and the ones using the sample mean? 

<!-- YOUR ANSWER HERE -->

Does it make sense to do a `t.test` of the Placebo sample against its own mean?

<!-- YOUR ANSWER HERE -->
