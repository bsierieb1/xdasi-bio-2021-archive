---
title: "T-test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Review

Today we will review concepts and practice different kinds of significance tests that are appropriate for different experimental scenarios. First, review from last time:

+ Random samples
+ Standard normal distribution
  - What is a $z$-score?
  - What are the quantiles?
  - How do you capture height vs. area? 
+ Sampling distribution of the sample mean
+ Standard error
+ Confidence intervals
+ What is a $z$-score?
+ $t$-statistics vs. $z$-statistics : when is each appropriate?
+ P values : one- vs. two-sided
+ What does qt(0.975) represent?


## One-sample, paired-sample, and two-sample $t$-tests

### The data

Here will discuss a simple case study where a drug has been provided to 10 random patients (or test subjects) and compared to the effect of a placebo pill given other random patients. Measurements were collected for each condition to answer the question:

#### Is there a significant difference between the control subjects and the those who were given the drug?

```{r}
# the population
Placebo_pop = read.csv("placebo_pop", header = FALSE)
Placebo_pop = Placebo_pop[,1]

# sample data
Placebo = c(54,51,58,44,55,52,42,47,58,46)
Drug = c(54,73,53,70,73,68,52,65,65,60)
```


### Confidence Interval

The standard error is very helpful because it gives us an idea of how close our data are to the actual mean. We can use the SE to help define **Confidence Intervals (CI)** of what the the actual population mean is.

Here, we assume that we have random samples and that the measurements are normally distributed in the population. The estimated distribution of the sample means will then follow a $t$-distribution, which is similar to a standard normal distribution but with heavier tails. As the sample size increases, the tails of the $t$-distribution become smaller and it resembles closely the $Z$-distribution.

Let's take our Drug sample and try to calculate our 95% CI for the mean of the population, assuming that the drug has no effect.

+ Estimate the mean and SE for the Drug sample.
+ Use the `qt()` *quantile function* to identify the critical $t$-score for the the 95% CI with the appropriate $df$.
+ Use the estimated mean and $t_{crit}$ to get our 95% CI.

```{r}

```

We can check our answer by using the `t.test()` function. Do this for both the Drug and the Placebo samples. What do you conclude?

```{r}

```


### One-sample $t$-test using an estimate for the population mean

Now let's pretend we don't know the population, so that Placebo is just a sample from some unknown population. Since we don't know the true $\sigma$, we will estimate it by calculating the $sd$ of the sample. Note that this only works if the population is truly normally distributed.

Calculate the 95% CI for the true Placebo mean using the Placebo sample.

```{r}

```

Let's test our estimates for the Placebo population mean using a $t$-test. We have to provide $\bar{x}$ as the estimate for the true population mean since we are pretending that we don't know $\mu$.

```{r}

```

How do these results differ from the previous test using the true population mean?


### Difference between sample means - two-sample $t$-test

Now let's start to consider that Placebo and Drug groups really are two separate populations. Now we want to estimate (with 95% confidence) the true difference in the means of the two populations.

We already know that the distribution of the sample mean is normally distributed, and that the sum or difference of two normally distributed variables is also normally distributed. Therefore, the difference in sample means must be normally distributed.

When we consider the means of two samples, the SE of the difference is the square root of the sum of the SE of each sample:

$$\sigma_{\mu_{1}-\mu_{2}}={\sqrt{\frac{\sigma_{1}^2}{n_1}+{\frac{\sigma_{2}^2}{n_2}}}}$$

Compute the SE of the difference below.

```{r}

```

#### 95% CI

We can calculate the 95% confidence interval by using the $t$-distribution. In this case, the degrees of freedom is the sum of the total values minus the number of samples ($df = 20-2$).

```{r}

```

Notice that the confidence interval does not span 0, which means it is extremely unlikely that the true difference between the Drug and Placebo populations is 0.

Let's check is this is true.

We can look at the $t$-statistic ( which is the equivalent to the Z-score for a normal distribution ). Compute the $t$-score for the difference in the means.

```{r}

```

#### Two-sample $t$-test

Now we can calculate the $p$-value as to whether our null hypothesis is true. The **null hypothesis** is that the difference between the two populations is 0. We know this because if we were to plot a distribution from random variables with the same mean, we would expect to get a difference of 0.

$$ H_o : \mu_{Drug} = \mu_{Placebo} $$
$$ H_A : \mu_{Drug} \ne mu_{Placebo} $$

First, calculate the test statistic by hand, then confirm it using the `t.test()` function.
```{r}

```

#### Paired $t$-test

What if the data are taken from the **same individuals**? For example, we could test the same patients before and after treatment. In this kind of experimental design, we say that the data are **paired**. If there is no difference between the two measurements for each individual -- for example, a new drug for blood pressure has no measurable benefit -- then we would expect that our before and after values would be about the same on average. 

The **paired $t$-test** is performed in the same way as the one-sample $t$-test, except we use the **mean difference between paired measurements** from the two samples, $\bar{X}_D$, to compute a test statistic. Our **null hypothesis** is usually that the mean difference, $D_o$, is zero (we could set it to something else if our null hypothesis is that the difference between them is something else ...).

For paired data, we assume that the two sets of measurements are arranged in the same order as the corresponding individuals (because we have good record-keeping practices!) The $t$-statistic is:

$$ t^* = \frac{\bar{X}_{D} - D_0}{\frac{s_{D}}{\sqrt{n}}} = \frac{\bar{X}_{D} - D_0}{SE_{D}}  = \frac{\sqrt{n}}{s_D}(\bar{X}_{D} - D_0) $$

where $\bar{X_D}$ is the mean of the pairwise differences, $s_D$ is the standard deviation of the pairwise differences, and $D_0$ is what we are testing (which in this case is $0$). 

To compute the test statistic, we can simply subtract one vector from the other to obtain pair-wise differences for each individual, take the mean, and divide by the standard error. We then find the $p$ value in the usual way.

Study the above equation to convince yourself that the form of the $t$-statistic above is the same as that for a one-sample $t$-test, substituting $\bar{X}_D$ for $\bar{X}$, $D_o$ for $\mu_o$, and $s_D$ for $s$. 

Do this by hand first, and then using the `t.test()` function.

```{r}

```

Notice that the result from the $t$-test with the paired option gives the same result as the one-sample $t$-test using the Placebo sample to estimate the true population parameter $\mu_{Placebo}$!

```{r}

```
