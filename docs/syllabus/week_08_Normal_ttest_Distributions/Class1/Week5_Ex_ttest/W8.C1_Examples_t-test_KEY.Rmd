---
title: One-sample t-tests

output:
  html_document:
    toc: yes
    toc_depth: 4
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse = TRUE)
```


## The data

Here will discuss a simple case study where a drug was provided to 10 random patients (or test subjects) and compared to the effect of a placebo pill given 10 other random patients. Measurements were collected for each condition to answer the question:

*Is there a significant difference between the control subjects and the those who were given the drug?*

First we will pretend that we have a large population of of people were given a placebo (the Population). Let's look at the data:

```{r}
# the population
Placebo_pop = read.csv("placebo_pop", header = FALSE)
Placebo_pop = Placebo_pop[,1]

# sample data
Placebo = c(54,51,58,44,55,52,42,47,58,46)
Drug = c(54,73,53,70,73,68,52,65,65,60)
```


## Placebo population distribution

```{r}

## check the parameters from the true population
paste("Pop. mean = ",round(mean(Placebo_pop),3),
      " ; Pop. SD = ",round(sd(Placebo_pop),4))

## visualize the results
hist(Placebo_pop, breaks=20, freq=FALSE)
xfit = seq(40,60,length=100)
yfit = dnorm(xfit,mean=mean(Placebo_pop),sd=sd(Placebo_pop))
lines(xfit,yfit,col="blue",lwd = 2)

```

## Confidence Intervals for the Population Mean

The standard error is very helpful because it gives us an idea of how close our data are to the actual mean. We can use the SE to help define **Confidence Intervals (CI)** for the the actual population means from which the samples were drawn.

Here, we assume that we have random samples and that the measurements are normally distributed in the population. The estimated distribution of the sample means will then follow a $t$-distribution, which is similar to a standard normal distribution but with heavier tails. As the sample size increases, the tails of the $t$-distribution become smaller and it resembles closely the $Z$-distribution.

Let's calculate the 95% CI for the mean of each population that the samples are drawn from.

+ Estimate the mean and SE for the Placebo and Drug samples.
+ Use the `qt()` *quantile function* to identify the critical $t$-score for the the 95% CI with the appropriate $df$.
+ Use the estimated means, SEs, and $t_{crit}$ to get the 95% CI's for the two samples.

```{r}
## t_critical: # 2.26 = t-score for 97.5% area
# Placebo and Drug are the same length so this works for both
t_0.975 = qt(p = .975, df = length(Placebo)-1)
paste0("t_crit = ",round(t_0.975,4))  # 2.26 = z-score for 97.5% area

## placebo
Pmean = mean(Placebo)
Pse=sd(Placebo)/sqrt(length(Placebo))

# 95% CI
c(Pmean - Pse * t_0.975, Pmean + Pse * t_0.975)

## drug
Dmean = mean(Drug)
Dse=sd(Drug)/sqrt(length(Drug))

# 95% CI
c(Dmean - Dse * t_0.975, Dmean + Dse * t_0.975)
```

We can check our answers by extracting the CI estimates from the `t.test(...)` function. You can inspect the `t.test` object using `str()`; it's a list containing a bunch of information about the results of the $t$-test. The precise expression to get the CI is `t.test()$confint[1:2]`. 

Do this for both the Drug and the Placebo samples. Are these the same as what you calculated above?

```{r}
t.test(Placebo, mu=mean(Placebo_pop))$conf.int[1:2]
t.test(Drug, mu=mean(Placebo_pop))$conf.int[1:2]
```

Using the rules of thumb given in your textbook, can you conclude by eye that the samples come from different populations? Why or why not?

<!-- YOUR ANSWER HERE -->

What do you conclude about the two samples relative to the true population mean and compared to each other?


## One-sample $t$-test using an estimate for the population mean

Now let's pretend we don't know the population, so that Placebo is just a sample from some unknown population. Since we don't know the true $\sigma$, we will estimate it.

First look at the mean and SD of the Placebo population and the Placebo sample.

```{r}
 
## check the parameters from the true population
paste("Pop. mean = ",round(mean(Placebo_pop),3),
      " ; Pop. SD = ",round(sd(Placebo_pop),4))

## check the parameters from the estimated population
paste("Pop. mean (est) = ",round(mean(Placebo),3),
      " ; Pop. SD (set) = ",round(sd(Placebo),4))

```

Do these look pretty different? Why?

## One-sample t-test with simulated population data

In order to perform a one-sample $t$-test, we technically need to have some data from a control population for comparison, so that we can test our samples against the true population mean. Here, however, we do not have the population data, so what do we do? We can actually reverse engineer a control sample population, using our best estimates of the population parameters using the control sample we do have.

Recall from Chapter 11 that our best estimate for the true population mean $\mu$ is the sample mean $\bar{Y}$. Our best estimate for the true population variance (Section 11.5) is:

$$ \sigma^2 = \frac{(n-1)s^2}{\chi^2} $$

That looks pretty simple, but what do we use for Chi-squared? If we were computing a 95% CI for the population SD, we would use $\chi_{\alpha=0.025}$ and $\chi_{\alpha=0.975}$. Instead, here we want a representative value from the sampling distribution of $\chi^2$, so we will use the 50th quantile (the midway point in the density distribution).

Using this information, we can sample from a normal distribution with our estimated parameters and pretend that this is our control population data. It will probably be closer overall to the Placebo sample than the original population, since we have to use the sample statistics to estimate the population parameters, but that's ok for now.


```{r}
## estimate the population parameters

# mean
pop_mean_est = mean(Placebo)

# s.d. -- use the sample variance and the Chi-square distribution
# sigma^2 = df * s^2 / chisq_est(0.5,9)
# a) compute chi-squared estimate
chisq_est = qchisq(0.5, length(Placebo)-1)

# b) compute pop sd estimate
pop_sd_est = sqrt( (length(Placebo)-1)*sd(Placebo) / chisq_est )

# check the estimated parameters
paste("Chi-square = ",round(chisq_est,4),
      " ; Pop. SD est. = ",round(pop_sd_est,4))

paste("Population mean estimate = ",round(pop_mean_est,3))

## simulate a population using the estimated population mean and sd
pop = rnorm(1000,mean=pop_mean_est,sd=pop_sd_est)

## visualize the results
hist(pop, breaks=20, freq=FALSE)
xfit = seq(40,60,length=100)
yfit = dnorm(xfit,mean=pop_mean_est,sd=pop_sd_est)
lines(xfit,yfit,col="blue",lwd = 2)
```

What's the difference between the true population parameters and the simulated population parameters?

```{r}

## check the parameters from the true population
paste("Pop. mean = ",round(mean(Placebo_pop),3),
      " ; Pop. SD = ",round(sd(Placebo_pop),4))

## check the parameters from the simulated population
paste("Pop. mean (simulated) = ",round(mean(pop),3),
      " ; Pop. SD (simulated) = ",round(sd(pop),4))

```


Calculate the 95% CI for the true Placebo mean using the Placebo sample.

```{r}

```

Let's test our estimates for the Placebo population mean using a $t$-test. We have to provide $\bar{x}$ as the estimate for the true population mean since we are pretending that we don't know $\mu$.

```{r}

```

How do these results differ from the previous test using the true population mean?


## One-sample $t$-test

Run a $t$-test for each sample against:

+ the mean of the simulated control population, and
+ the population mean estimate from just the Placebo sample.

```{r}
# Placebo sample
t.test(Placebo, mu=mean(Placebo_pop))
t.test(Placebo, mu=mean(Placebo))

# Drug sample
t.test(Drug, mu=mean(Placebo_pop))
t.test(Drug, mu=mean(Placebo))
```

How similar are the results between the tests using the "true" (simulated) population mean and the ones using the sample mean? 

<!-- YOUR ANSWER HERE -->

Does it make sense to do a `t.test` of the Placebo sample against its own mean?

<!-- YOUR ANSWER HERE -->
