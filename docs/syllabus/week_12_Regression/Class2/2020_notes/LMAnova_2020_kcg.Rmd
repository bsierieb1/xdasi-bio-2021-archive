---
title: "ANOVA and linear models"
author: "Manpreet S. Katari (ed. KCG)"
output:
  html_document: default
  pdf_document: default
---

## Comparison of ANOVA with linear models

### Two samples with the same variance

Let's generate two sample groups, *ctrl* and *trt*, by taking random samples from normally distributed data.


```{r, collapse=TRUE}
# Select 5 random numbers from distribution where *mean*=0 and *sd*=1
ctrl=rnorm(5)

# Select 5 random numbers from distribution where *mean*=5 and *sd*= 1
trt = rnorm(5, mean=5)

# Combine both samples into one vector
data = c(ctrl,trt)

# Define how the data should be grouped
expgroups = factor(rep(c("ctrl", "trt"), each=5))

# Use a boxplot to show the distributions
boxplot(data ~ expgroups)
```


## Perform ANOVA

The model is written such that the dependent variable (data) is explained by the independent variable (expgroups). 

**NOTE: Two ways to perform an ANOVA analysis are to use either the `aov()` function or the `lm()` function.**

### ANOVA Method 1: `aov()`

NOTE: There are two equivalent ways look at a summary of an ANOVA using the `aov()` command, `summary(aov(data~expgroups))` and `summary.aov()`. Both of these expect an object of class `aov` as the argument.

```{r, collapse = TRUE}
aov(data~expgroups)

summary(aov(data~expgroups))  # same as `summary.aov()`
```

If the ratio $SSB/SSW = SumSq(expgroups)/SumSq(Residuals)$ is a lot different from 1, we can conclude that there is a difference between the groups. Here, the report tells us that the SSB is ~60, and the SSW ~10, so this looks pretty good.

But to get a proper $p$-value, we actually want the **F-statistic**, which uses the **mean sums of squares**: $F = MS_B / MS_W$. To get the mean sums of squares, we also need the degrees of freedom. The between-groups df are [# groups - 1], and the df for the residuals (within groups) are [# data points - # groups]. So here, $F = (SSB/df.exp) / (SSW/df.res) = (SSB/1) / (SSW/8)$.

### ANOVA Method 2: `lm()`

The `lm()` function makes a linear model of the data as explained by `expgroups`. The `expgroups` factor is the independent variable, and `data` is the dependent variable.

NOTE: As with the ANOVA summary function, there are two different ways to get a summary of a linear model object produced by the `lm()` function: `summary(linear_model)` and `summary.lm(linear_model)`. Both methods expect an object of class `lm` as the argument.

```{r, collapse = TRUE}
datalm = lm(data ~ expgroups)

summary(datalm)  # same as `summary.lm()`
```

Using the `anova()` function on the linear model (object) gives you the same information as the summary of `aov()`:

```{r}
anova(datalm) # same as summary(aov(data ~ expgroups))
```

### Comparing results of ANOVA and linear modeling

The *null hypothesis* of linear modeling is that the **slope** of the line is 0. In contrast, the *null hypothesis* of the ANOVA is that the **variance** of all of the data combined is the same as the variance of the individual groups.

In other words, the *baseline* for `lm` is the control, whereas for ANOVA there is no baseline; you simply look at all the *differences* between the groups. Concomitantly, a $t$-statistic compares **means**, whereas an $F$-statistic compares **variance**. For a linear model, the $t$-test compares the observed slope vs. the expected slope of $\beta=0$, i.e. a horizontal line. 

Despite these methodological differences, **notice that the $p$-values are the same.** This is because in both cases, the *variance* is still needed to make a good estimate of *significance*. As we saw before, for a model with only one explanatory variable, **the $F$-statistic is equivalent the square of the $t$-statistic**. Comparison with each reference distribution gives the same $p$-value.

We can compute the $t$-statistic and $p$-value from the linear model "by hand" using the following:

+ `coef(linear_model)` -- the estimated coefficients for the slope and intercept
+ `sqrt(diag(vcov(linear_model))` -- the standard errors of the estimated model parameters, which are obtained from the diagonal elements of their covariance matrix

```{r, collapse=TRUE}
# coefficients
coef(datalm)

# estimated error
sqrt(diag(vcov(datalm)))

# t-statistics from the linear model
tstats <- coef(datalm) / sqrt(diag(vcov(datalm)))
tstats

# p-value
2 * pt(abs(tstats), df = df.residual(datalm), lower.tail = FALSE)

# t-statistic squared is the same as the F-statistic
tstats^2
```


### Normally distributed data for three groups

![](anova.PNG)

Let's make another group and call it `trt2` with the same sample size, but a different mean. Then, let's combine this with our previous data and make a new linear model.

```{r, collapse=TRUE}
# new treatment group
trt2= rnorm(5,mean=10)

# combine data
data = c(ctrl,trt,trt2)
expgroups = factor(rep(c("ctrl", "trt", "trt2"), each=5))

# make a boxplot
boxplot(data ~ expgroups)

# make a new linear model
datalm = lm(data ~ expgroups)
summary(datalm)
```

For this model, "Estimate" gives the slope of the line between the control and each experimental group. This is around 5 comparing ctl-trt1 and ~10 comparing ctl-trt2. This is just the difference between the means.

We get $p$-values for the individual groups, and an overall $p$-value for the model.

Let's compare this to the ANOVA result:

```{r}
summary(aov(data ~ expgroups))
```

This summary tells us that the total variance among groups is a lot bigger than the variance within groups, but we don't get any information about the significance of each group on its own.


### Tukey's honest significant difference test

We can use "Tukey's honest significant difference" test, **TukeyHSD**, to help us figure this out. The TukeyHSD shows the magnitude of the differences between all pairwise combinations of the groups, so you can tell which groups are responsible for any major differences between them.

```{r}
TukeyHSD(aov(data ~ expgroups))
```

Again, performing the `anova()` function on the linear model gives us the same results as the summary of `aov()`. However, we cannot perform `TukeyHSD()` on the results of the `anova()` function. (Why is this?)

```{r}
anova(datalm)
```


### What if trt2 is the same as trt?

```{r, collapse=TRUE}

# new dataset with two treatment samples drawn from the same distribution
trt2= rnorm(5,mean=5)
data = c(ctrl,trt,trt2)
expgroups = factor(rep(c("ctrl", "trt","trt2"), each=5))
boxplot(data ~ expgroups)
datalm = lm(data ~ expgroups)
summary(datalm)
```

Now let's perform an ANOVA using `aov()` and run Tukey's HSD test.

```{r}
summary(aov(data~expgroups))
TukeyHSD(aov(data~expgroups))
```

Notice that for the ANOVA, the differences between the groups are the same as the slopes between the control and the treatment groups in the linear model!


### What if trt2 is same as ctrl?

```{r}
# new dataset with one treatment samples drawn from the same distribution as the control
trt2= rnorm(5,mean=0)
data = c(ctrl,trt,trt2)
expgroups = factor(rep(c("ctrl", "trt","trt2"), each=5))
boxplot(data~expgroups)
datalm = lm(data ~ expgroups)
summary(datalm)
```

Compare this with the results of an ANOVA and TukeyHSD.

```{r, collapse=TRUE}
summary(aov(data~expgroups))
TukeyHSD(aov(data~expgroups))
```

    
