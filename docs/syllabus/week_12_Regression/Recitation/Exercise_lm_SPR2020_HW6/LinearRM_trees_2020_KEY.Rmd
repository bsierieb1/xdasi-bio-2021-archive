---
title: "Linear regression exercise"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Linear modeling

### Q0: The dataset

A dataset called `trees` comes packaged with R and is always available in your workspace.  This dataset contains measurements of felled timber from 31 cherry trees:

+ ``Height`` (in feet)
+ ``Volume`` (in cubic ft)
+ ``Girth`` (actually it's the tree diameter, measured 4'6" above the ground (in inches)

Check out the dataset to get a feel for what it looks like.

```{r}
# inspect the dataset
head(trees)
str(trees)
```


### Q1: Create linear models of Volume vs. predictor variables

We are interested in predicting the tree Volume from the measured height and diameter. First, let's create the simplest possible linear models, using one or both predictive variables.

Note: Remember that there are two equivalent expressions for specifying the formula: you can either refer to the columns of the columns directly using the `$` notation, or you can specify the data to be used with `data=` and just use the column names in the formula.  The latter is a bit easier to type and to read.

```{r}
## lm #####################################################################
# create linear models & check summaries

# Height
# tree_lm = lm(trees$Volume ~ trees$Height) # equivalent, harder to read
tree_lm1 = lm(Volume ~ Height, data = trees)
summary(tree_lm1)

# Girth
tree_lm2 = lm(Volume ~ Girth, data = trees)
summary(tree_lm2)

# Height + Girth
tree_lm3 = lm(Volume ~ Height + Girth, data = trees)
summary(tree_lm3)
```

What do the models tell you about how much of the variation in Volume are explained by Height and Girth? You should consider the following in your evaluation:

+ Residuals -- How big are they?
+ Coefficients -- 
+ R-squared -- How much of the variation in the data are explained by the Explanatory variable? Does this look encouraging?

<!-- YOUR ANSWER HERE -->


Which model seems best at first glance, and why? Is there anything troubling about these models? Why or why not?

<!-- YOUR ANSWER HERE -->


Based on the statistics from the linear regression, how would you evaluate the goodness-of-fit? Do the data violate any of the basic assumptions for linear modeling? 
<!-- YOUR ANSWER HERE -->


### Q2: Simple plots of Response vs. Explanatory variables

Before deciding upon a model, we need to check whether any of the basic assumptions of linear models are violated. What are these assumptions?

<!-- YOUR ANSWER HERE -->


Let's draw some simple plots to get an intial feel for how well the models fit the assumptions of linear models. 

First, explore the data by superimposing a smoothened line over the datapoints using `geom_smooth()`. 

*Note: The default method for smoothing is called `LOESS`, which stands for "locally estimated scatterplot smoothing". This method uses locally weighted least-squares across a sliding window to compute the best fit for each datapoint. The "smoothing parameter", a.k.a. the "bandwidth", determines how much of the data is used at each step.*

Next, remake the plots to show the best-fit regression line using `stat_smooth()` with the "lm" method. This will automatically show the 95% CI around the regression line.

```{r}
# load the ggplot library
library(ggplot2)

###################################################################################
## 1) create a plot of Volume ~ Height using ggplot

# a) simple plot -- show the data points + a smoothened line
ggplot(trees, aes(x=Height, y=Volume))+
    geom_point() +
    geom_smooth()

# b) plot datapoints with linear regression line, in RED
ggplot(trees, aes(x=Height, y=Volume))+
    geom_point() +
    stat_smooth(method="lm", col="red")


###################################################################################
## 2) plot Volume ~ Girth

# a) simple plot -- datapoints and smoothened line
ggplot(trees, aes(x=Girth, y=Volume))+
    geom_point() +
    geom_smooth()

# b) plot datapoints with linear regression line, in RED
ggplot(trees, aes(x=Girth, y=Volume))+
    geom_point() +
    stat_smooth(method="lm", col="red")

```

How do these plots look to you? Do you notice anything funny about the LOESS curve for Volume vs. Girth?

<!-- YOUR ANSWER HERE -->


### Q3: Diagnostic residual plots

You can assess the quality of any linear model using a variety of plots that show different properties of the **residuals** (the distance from each data point to the fitted line).


#### Types of Diagnostic Plots

##### Residuals vs. Fitted

This plot looks at the distance of the data points to the predicted line. There are several things to look for on this plot:

+ **Small residuals?** Ideally, the points should be close to the dotted line (where the residual is 0). Small residuals reflect a good fit to the data.
+ **Uniform residuals?** The magnitude of the residuals should be about the same across the range of measured x-values (equal variance).
+ **Outliers?** Outliers are numbered accoring to their index in the data frame to call attention to them.
+ **Linear relationship?** If the relationship is linear, then the fitted line should be relatively straight. If this is not the case, then applying some transformations to the predictor and/or response variables may help make the data more linear.

##### Normal QQ plot of residuals

This plot is a quantile-quantile plot that shows whether the Residuals are normally distributed. Again, the best case scenario is that the points fall on the dashed line.

##### Scale-location plot

This plot shows the **variance** in the residuals to the regression line. Ideally, the line should be horizontal, indicating that the variance is evenly distributed across all the datapoints. **Homoscedasticity**, or equal variance, is an assumption of certain statistical tests.

##### Residuals vs. Leverage

**Cook's distance** is calculated by removing a specific data point and checking how much the model has changed. The greater the Cook's distance, the more influential that point is on the model. The dashed horizontal line corresponds to the ideal situation in which each datapoint has little effect on the overall model. Outliers can be very disruptive, so this is a good way to determine if there are indeed outliers that have a strong negative impact on the linear model.

#### Examine the residuals plots

Here we will use a `ggplot2` companion package called `ggfortify`, which takes the normal base R graphics and converts them into ggplot graphs. This package defines the function `autoplot()`, which automatically creates one or more standard plots that are tailored to specific object classes. Here we will apply `autoplot` to our alternative linear models.

```{r}
# load the ggfortigy library
# install.packages("ggfortify") -- uncomment if you haven't already loaded the package
library(ggfortify)

# use autoplot to check on the different models
autoplot(tree_lm1)
autoplot(tree_lm2)
autoplot(tree_lm3)
```

Examine the plots for the three models and discuss the following:

**Model 1: Volume ~ Height**

Comment specifically on the uniformity of variation as revealed by the Residuals vs. Fitted and Scale-Location plots. What do the patterns reveal?

<!-- YOUR ANSWER HERE -->

**Models 2 & 3: Volume ~ Girth and Volume ~ Height + Girth**

Comment specifically on the Residuals vs. Fitted plot. What shape is the LOESS line? Can you think of any functions that resemble this pattern?

<!-- YOUR ANSWER HERE -->


Also consider the Scale-Location and Residuals vs. Leverage plots. What do you learn from these?

<!-- YOUR ANSWER HERE -->

Finally, discuss what you think the main dependencies in this dataset are, i.e. how influential each predictor is. The way you think about this will determine the downstream analyses that you will perform.

<!-- YOUR ANSWER HERE -->


### Q4: Data transformation

The above plots reveal that linear models of the raw data might not provide the best picture of the relationship of Height and Girth with Volume.

Reflecting upon the exercises above, and *your own knowledge of volumetric measurements*, can you think of a way to transform the data that might improve the model? Explain your reasoning. If you have a particular mathematical formula in mind, please include it in your answer.

*Hint: You may want to play around with a few transformations on your own to see what might work best (do not include your exploratory analysis here). Since this is not a trick question, stick to common transformations like log / power functions and quadratics (square / square root). You can also just answer this question based on first principles.*

<!-- YOUR ANSWER HERE -->


Below, create a linear model using the transformation you think works best. Since Height made little contribution to the model, just use Girth as the Predictor variable.

*Note: Please transform the Response variable rather than the Predictor variable(s) in the linear model. This will allow you to predict Volume from any set of new measured values for Girth, which you will be asked to do below. Making predictions on new data, based on models generated from training data, is a common task in data analysis.*

```{r}
# make a linear model and look at the summary
model = lm(sqrt(Volume) ~ Girth, data = trees)
summary(model)

# use ggplot to make a plot of the transformed data
# using `stat_smooth` to show the regression line
ggplot(trees, aes(x=Girth, y=sqrt(Volume))) +
    geom_point() +
    stat_smooth(method="lm")

# check out the diagnostic plots for the residuals
autoplot(model)
```

How have your stats and diagnostic plots changed? What features of the particular transformation you have chosen do you think correct the problems you saw before?

<!-- YOUR ANSWER HERE -->


### Q5: Prediction and Confidence Intervals

First, apply the `predict.lm()` function to your best linear model to predict the Volume across a range of values for Girth. Read the documentation for this function, and include lower and upper *Prediction Intervals* in the model. 

For the predictions, generate 20 random numbers chosen from a uniform distribution that spans the range of values in the original dataset.

```{r}
test_set = data.frame(Girth = sort(runif(20, min=min(trees$Girth),
                                             max=max(trees$Girth))))
pred_data = data.frame(predict.lm(model, newdata = test_set, interval = "predict"))
test_set = cbind(test_set,
                 VolumeSqrt = pred_data$fit,
                 lwr = pred_data$lwr, 
                 upr = pred_data$upr)
test_set

```

Now, use `ggplot2` to plot the original data, the regression line including lower and upper confidence intervals, the prediction intervals for the model, and the predicted values for the test data.

```{r}

ggplot() +
    stat_smooth(method="lm", data = trees, aes(x=Girth, y=sqrt(Volume)), 
                col="darkgray") +
    geom_point(data = trees, aes(x=Girth, sqrt(Volume)), 
               col="midnightblue") +
    
    geom_point(data = test_set, aes(Girth, VolumeSqrt), col="red") +
    geom_line(data = test_set, aes(Girth, lwr),
              color = "red", linetype = "dashed") +
    geom_line(data = test_set, aes(Girth, upr),
              color = "red", linetype = "dashed") +

    ggtitle("trees Volume prediction -- training data (black) + test data (red)") +
    xlab("Girth") + ylab("sqrt(Volume)")
```

And voil√†! The End. ;-)
