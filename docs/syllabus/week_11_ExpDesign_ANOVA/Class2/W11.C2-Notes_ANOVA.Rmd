---
title: "Analysis of Variance (ANOVA)"
subtitle: XDASI Fall 2021
date: "November 11, 2021"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    toc: yes
    toc_depth: 3
---

<!-- initial version from Manny Katari ; edited by KCG -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Suggested reading

+ Whitlock & Schluter, Ch 15 and  <a href="https://whitlockschluter3e.zoology.ubc.ca/RLabs/R_tutorial_ANOVA.html" target="blank">  online lab </a>
+ Online tutorials
  + Antoine Soetewey (UCLouvain, Belgium) -  <a href="https://statsandr.com/blog/anova-in-r/#anova-in-r" target="blank"> Stats and R blog: ANOVA (2020-10-02) </a>
  + (Steven Doogue, 2019-07-09) - <a href="  https://statsandr.com/blog/anova-in-r/#anova-in-r" target="blank"> Chapter 7.1: One-way ANOVA </a>

## Introduction

We've previously used $t$-tests and non-parametric methods to compare two samples. What if we need to compare more than two samples? The problem with just performing multiple $t$-tests is that each test has a certain Type I error rate, and when we perform multiple tests this error simply compounds. This issue can be addressed by ***correction for multiple hypothesis testing***, which we will cover later.

If doing all pairwise $t$-tests between groups is not a good approach, what can we do instead? ANOVA to the rescue! ANOVA extends $t$-tests to more than two groups by allowing the comparison of the means between multiple groups. However, instead of simply comparing the groups using the difference in their means and their SD, ANOVA compares the ***overall variation between groups*** to the ***variation within each group***. If the overall variation is significantly greater than the individual variation, then we consider the groups to be different.

The simplest form of ANOVA is ***one-way ANOVA***, which we will discuss here. 

## Assumptions

Like $t$-tests, ANOVA assumes that the distributions of the samples are relatively normal, and also that their variances are similar. When these do not hold, a non-parametric analog ANOVA can be performed called the ***Kruskall-Wallace test***.

The basic idea is illustrated in Figures 15.1-1 and 15.1-2:

```{r, echo=FALSE, fig.align='center'}
library(cowplot)
ggdraw() + 
  draw_image("Images/WS _Fig_15.1-1.png", width = 0.30) + 
  draw_image("Images/WS _Fig_15.1-2.png", width = 0.60, x = 0.4)
``` 

To determine whether there is a significant difference between groups, the variation is decomposed into three parts:

+ Total variation with respect to the grand mean
+ Variation between group means and the grand mean
+ Variation within each group w.r.t. its group mean

These differences are calculated using the ***sum of squares*** of the difference between each data point and the mean used for comparison:

+ Total variation: $SST = \sum_{i=1}^n{}{}$

The calculations that are involved are summarized in Table 10.1 from Ken Aho's book:

<div align="center">
![](Images/Aho_ANOVA_SS.png){width="70%"}
</div>

The ***total sum of squares (TSS)*** equals the ***within-group SS (SSW)***, also called the ***error SS (SSE)***, plus the ***between-group SS (SSB or SSG)***:

$$SST = SSW + SSB$$

As usual, we need to take into account the ***degrees of freedom***, which is just the number of data points in each equation minus one (the mean used for comparison).


## The $\chi^2$ distribution

Since we are measuring differences using ***sums of squares***, the differences will follow a $\chi^{2}$ distribution, which represents the **sum of squared random values** selected from a **normal distribution**. The degrees of freedom, $k$ is simply the number of random values.

$$Q = \sum\limits_{i=1}^{k} Z_{i}^{2}$$

#### $\chi^2$ with different sample sizes

Let's simulate some data to see what it looks like when k = 10, 5, and 1 and the values are retrieved from a standard normal distribution. (We have also done this in a previous class.)

```{r echo=F}
#################

# Sample size k = 10 ==> Chi-squared degrees of freedom = 10
chik=10
normval=numeric()
for (i in 1:10000) {  # repeat this 10000 times
  
  # Randomly sample k items from the standard normal distribution
  # then take the sum of squared values of the random samples
  normval[i]=sum(rnorm(chik)**2)
}

# make a histogram of the results
hist(normval, breaks=100, # make 100 bins on the x-axis
     col=rgb(1,0,0,.25),  # make it red, with 25% transparency
                          # this allows us to show multiple histograms on the same plot
     ylim=c(0,2000),      # set the limits of the y-axis
     main = "Chi-square samples from the standard normal dist"
    )
# Sample size k = 5 ==> Chi-squared degrees of freedom = 5
chik=5
normval=numeric()
for (i in 1:10000) {
  normval[i]=sum(rnorm(chik)**2)
}

# the `add` directive adds the new histogram to the old plot
hist(normval, breaks=75,          # reduce the number of bins since this distribution
                                  # is squished to the left
     col=rgb(0,1,0,.25), add=T)   # green, with 25% transparency

# Sample size k = 1 ==> Chi-squared degrees of freedom = 1
chik=1
normval=numeric()
for (i in 1:10000) {
  normval[i]=sum(rnorm(chik)**2)
}

hist(normval, breaks=50,          # use even fewer bins
     col=rgb(0,0,1,.25), add=T)   # blue, with 25% transparency
```

It is clear to see that as $k$ increases, the distribution begins to look like a **normal distribution**. 

This only happens when the **sample size is at least 10**, which is why it is not recommended to use the $\chi^{2}$ test for small values of $k$ (<10).


## The $F$-statistic

To ***compare two $\chi^{2}$ distributions***, we can simply take a ***ratio*** of them (taking into account their respective degrees of freedom). 

This distribution is called an **$F$-distribution** and the _**ratio**_ is the **$F$-statistic**. (The $F$-distribution is named after the statistician Ron Fisher.)

#### F-distribution for samples from the same population

Let's first see what it would look like if the the means of the two populations that are being sampled are **equal**.

```{r echo=F}

chik=10
normval=numeric()
normval2=numeric()
for (i in 1:10000) {
   normval[i]=sum(rnorm(chik)**2)
   normval2[i]=sum(rnorm(chik)**2) 
  
}

# since the values are sampled from the same distribution, 
# the ratios are centered around 1
hist(((normval/(chik-1))/(normval2/(chik-1))),
     main = "Histogram of F-distribution",
     breaks=100, col=rgb(1,0,0,.25))
```

Note that depending on how close most of the density is to the mean, the heights of two histograms will vary, even if they are generated using exactly the same procedure. Run the above code several times and see how the results change each time.


#### F-distribution for populations with different means

What if the means of our normal distributions are different? 

We can make a second histogram showing the same ratio for data sampled from two normal distributions with different means: the standard normal and a normal distribution with mean = 2 and sd = 1.

Now, the ratio of the sums of the two samples will look quite different. Let's try this and superimpose the two histograms for comparison.

```{r echo=FALSE}
# same as above -- sample two sums of squares from the same dist
chik=10
normval=numeric()
normval2=numeric()
for (i in 1:10000) {
  # take two sets of sample of size 10 from the standard normal distribution
   normval[i]=sum(rnorm(chik)**2)
   normval2[i]=sum(rnorm(chik)**2) 
  
}

# now plot the ratios of them
# note that we didn't bother to store the intermediate result
# since we don't need to do anything with it later.
hist(((normval2/(chik-1))/(normval/(chik-1))),
     main = "Histogram of F-distribution",
     breaks=100, ylim=c(0,1500), 
     xlim=c(0,30),
     col=rgb(1,0,0,.25))

## now compare sums of squares when the means are different
chik=10
normval=numeric()
normval2=numeric()
for (i in 1:10000) {
   normval[i]=sum(rnorm(chik)**2)
   normval2[i]=sum(rnorm(chik, mean=2)**2) 
  

}

hist(((normval2/(chik-1))/(normval/(chik-1))), 
     breaks = 500, col=rgb(1,1,0,.25), add=T)    # need a lot less breaks here

```

Remember that ***variance*** is essentially a sum of squares as discussed above. So now we have the ability to compare two different variances and use a statistic to determine if they are significantly different.
