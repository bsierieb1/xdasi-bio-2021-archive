---
title: "Non-parametric tests"
author: "Kris Gunsalus"
date: "10/20/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## What if the data are not normally distributed?

### Transformation

When the data do not look to be sufficiently normal, we can try performing tests on data that have been **transformed** using functions such as the `log()`, `sqrt()`, or `arcsin()`. This can make data look more normal, so that parametric tests may be performed on them.

#### Log transform

In biology it is common that multiple factors influence our measurements. The effects may be additive or multiplicative. We know from probability theory that to find the cumulative probability of several independent variables, we can multiply them (product rule). This type of data often gives rise to log-distributed measurements. Taking the log of these stretches out the small values and compresses the larger ones, rendering the data more normal-looking.

Many examples follow a log-normal distribution, such as exponential growth (cell count doubles with each division), systolic blood presssure, and the latency period for infectious diseases.

We can use th *Shapiro-Wilk* test to see whether the data follow a normal distribution. Let's investigate the data shown below for plasma triglyceride levels using:

+ histograms
+ a test for normality
+ a `t.test()` on the transformed data
+ the 95% CI for the data in their original units
  - we can compute by hand (hard), or extract from `t.test()$conf.int[1:2]` (easy)
  - don't forget to back-transform using `exp()`

*Note:* By default, the `log()` function uses the natural log. You can specify other bases with the `base` parameter; convenience functions such as `log10()` and `log2()` are also available.

```{r, collapse=TRUE}
#####################################################################
# plasma triglyceride levels in the population (mg/ml)
# borderline high = 2-4 vs. normal < 2
# testing before and after diet and exercise changes (expect a decrease)
pre = c(2.55,3.38,2.37,4.11,3.27,2.58,4.20,3.22,5.10,2.62,3.06,1.23,2.27,2.24,1.39,2.63,2.61,4.30,1.46,3.35,2.79,2.42,4.63,1.57)
post = c(1.59,3.51,1.44,2.32,1.75,1.67,1.90,1.37,2.72,1.80,2.40,2.01,2.41,1.38,1.18,4.31,2.09,2.32,2.63,2.25,1.71,2.01,1.95,3.46)
#####################################################################
```

```{r, collapse=TRUE}

# check distributions of original data with histograms (breaks = 10)
par(mfrow=c(2,2))
hist(pre,breaks=10)
hist(post,breaks=10)

# check distributions after log-transformation
hist(log(pre),breaks=10)
hist(log(post),breaks=10)

# Shapiro-Wilk tests
shapiro.test(pre)
shapiro.test(post)
```

What do you conclude from your inspection of the normality of the data?

<!-- YOUR ANSWER HERE -->


```{r, collapse=TRUE}

## do t-tests using the original and transformed data
t.test(post,pre,paired=T)
t.test(log(post),log(pre),paired=T)
```

What are the most striking differences in the results of the $t$-tests? Which $t$-test is more appropriate, and why?

<!-- YOUR ANSWER HERE -->


Compute the 95% CI for the post-treatment and pre-treatment samples. Do one of these by hand first, and then use the CI values from one-sample $t$-tests to get both of these automatically (don't forget to back-transform!)

```{r, collapse=TRUE}

## compute the 95% CI for the two samples in their original units

#####################################################################
# post-treatment -- by hand!
mean_prime = mean(log(post))
sd_prime = sqrt( sum( (log(post) - mean_prime)^2 /(length(post)-1)) )
se_prime = sd_prime/sqrt(length(post))
tcrit = qt(0.975,df=length(post))
tcrit
ci_post = c(mean_prime - tcrit*se_prime, mean_prime + tcrit*se_prime)
ci_post
ci_post_orig = exp(ci_post)
ci_post_orig

# post-treatment from t-test
t_post = t.test(log(post),mu=mean(log(pre)))
t_post
ci95_post_log = t_post$conf.int[1:2]
ci95_post = exp(ci95_post_log)
ci95_post

#####################################################################
# pre-treatment from t-test
t_pre = t.test(log(pre),mu=mean(log(pre)))
t_pre
ci95_pre_log = t_pre$conf.int[1:2]
ci95_pre_log
ci95_pre = exp(ci95_pre_log)
ci95_pre
```

Did you get the same CI for the post-treatment sample when computing by hand and using the `t.test()`? What can you conclude from the 95% CI's of the two samples?

<!-- YOUR ANSWER HERE -->


## Non-parametric (rank-based) tests

When the sample data do not follow a normal distribution, we can use tests that look at the data in terms of their ranks (or ranked differences) instead of their magnitudes. 

### Tests for paired data: Sign and Wilcoxon signed-rank tests

For paired data, our null hypothesis is that the paired differences between the groups should end up with about the same number being positive or negative. This works well when the two differences have a similar skew and variance.

#### Sign test

Instead of comparing **means** or mean differences, the **sign test** compares the **median** of a sample to a hypothesized value. For paired data, under the null hypothesis we would expect the medians of two samples to be the same. The hypotheses to be tested are:

$H_o$: The median difference between the groups is zero.

$H_A$: The median difference between the groups is NOT zero.

The sign test simply takes the signs of the paired differences and counts up the number of negative and positive deviations from zero. The $p$-value is then equivalent to the binomial probability for the observed number of negative deviations when the expected probability is 0.5.

The steps are:

+ Calculate the pairwise differences between the samples.
+ Count the number of nonzero differences $n$.
+ The test statistic, let's call it $k$, is the smaller number of the positive and negative differences.
+ Calculate the binomial probability of observing $k$ out of $n$ differences given an expected probability of $\pi = 0.5$.

Compute the binomial probability for the "pre" and "post" triglyceride samples. You can use the `ifelse()` function to find the lower number between the negative and positive differences.

```{r}
# sign test for the triglyceride data

# difference in the two samples
tri_diff = post - pre

neg_count = sum(tri_diff < 0)
neg_count
pos_count = sum(tri_diff > 0)
test_stat = ifelse(neg_count < pos_count, neg_count, pos_count)

# binomial probability of seeing neg_count or more
prob_obs = pbinom(test_stat, length(tri_diff), prob=0.5)

# total two-tailed probability
2*prob_obs
```


#### Wilcoxon signed-rank test

The **Wilcoxon signed-rank test** is similar to the sign test except that it takes the sum of all the signed ranks (see below), and it can easily be performed directly in R. 

One caveat of this test is that it assumes a symmetric distribution, which limits its applicability. The test is often used when this assumption is violated, so keep this in mind and try not to make the same mistake.

Here we will run the test on the log-transformed data, which is more symmetrical than the original data.

##### Procedure

The steps are:

+ Calculate the $N$ differences between the pairs.
+ Rank the *absolute* values of the $n$ non-zero differences. Assign the average if there is a tie.
+ Also record the sign of the differences, $sgn(x_{2i} - x_{1i})$ for $i \in \{1..N\}$.
+ Compute the test statistic $W$, defined as:

$$ W = \sum_{i=1}^Nsgn[sgn(x_{2i} - x_{1i}) * R_i] $$

```{r}
# compute paired log differences
log_tri_diff = log(post) - log(pre)
rank_diff = rank(abs(log_tri_diff))

# Calculate the Wilcoxon W statistic
w_stat = sum(sign(log_tri_diff) * rank_diff)
w_stat
```

##### Normal approximation

$W$ has a specific expected distribution under the null. The population mean and variance are:

$$ \mu_{W} = 0 ;\ \sigma_{W}^2 = \frac{n(n+1)(2n+1)}{6} $$

When the number of items sampled is more than ~20, we can use a normal approximation to compute a $z$-score, $z = W / \sigma_W$, from which we can obtain a $p$-value.

Below we use the normal approximation to compute a $p$-value for the log-transformed data.

```{r}
# w stat
n = length(log_tri_diff)
sigma_w = n*(n+1)*(2*n+1) / 6
z_w = w_stat / sqrt(sigma_w)
2*pnorm(z_w)
```

##### Wilcoxon signed-rank test in R

Use the `wilcox.test()` function to test the original and the log-transformed triglyceride data. Don't forget that we are doing paired tests here.

**Note**: The R implementation of this test uses something called a $V$-statistic, which is a little different than the $W$-statistic: it is the **sum of the ranks** assigned to the differences with a **positive** sign.  The test gives the same result when samples are entered in either order.

The normal approximation for the $V$-statistic has parameters:
$$ \mu = n(n+1)/4\ ;\ \sigma = n(n+1)(2n+1)/24$$

The $p$-values from both the $W$ and the $V$ are the same.

```{r}
# computing the V statistic
diff_neg = rank_diff[log_tri_diff < 0]
diff_neg
t_neg = sum(diff_neg)
t_neg
diff_pos = rank_diff[log_tri_diff > 0]
diff_pos
t_pos = sum(diff_pos)
t_pos

v_stat = min(t_neg,t_pos)
v_stat



# v stat
mean_v = n*(n+1)/4
sigma_v = n*(n+1)*(2*n+1) / 24
z_v = (v_stat - mean_v) / sqrt(sigma_v)
2*pnorm(z_v)
```

Run the test in R, using both the raw and the log-transformed data.

```{r}
# test raw data
wilcox.test(post, pre, paired=T)

# test log data
wilcox.test(log(post), log(pre), paired=T)
```

How do the manual calculations compare to the results of the R tests? Why might these differ?

<!-- YOUR ANSWER HERE -->

What do you notice about the differences in the results for the non-transformed vs. the transformed data? Which version is more appropriate?

<!-- YOUR ANSWER HERE -->


### Unpaired data: Mann-Whitney-Wilcoxon rank-sum test

If our data are **not normal** and also **not paired**, we can use a **Mann-Whitney U-test** or the equivalent **Wilcoxon rank-sum** test. Some people therefore like to refer to this test as a **Mann-Whitney-Wilcoxon** test. The test is implemented in R as `wilcox.test()`, with the (default) unpaired option.

This test is simlar to the paired signed-rank test, but instead of measuring paired differences and counting positive and negative differences, we compute the sum of ranks for each group in the combined data. This makes intuitive sense because if the data are drawn from a homogeneous population, we would expect the values of the measurements from two random samples to be relatively well interleaved (as if we had shuffled a deck of cards). So, we would expect that the overall sum of ranks should be about the same. 

#### Procedure

The hypotheses to be tested are:

$H_o$: The sample distributions are the same.

$H_A$: The sample distributions are NOT the same.

The steps are:

+ Assign ranks to the combined data, from lowest to highest.
+ Assign ties the midrank between them (the average of the ranks).
+ Compute the sum of ranks $R_1$ for Sample 1.
+ Compute the $U$-statistic for Samples 1 and 2.
+ The sum of the ranks that is higher in magnitude is the test statistic (these will vary depending on the number of individuals in each sample).
+ Compute the $p$-value using the quantile function for the $U$-distribution.

**Note**: The sum of ranks for a sequential set of numbers starting with 1 is $N(N+1)/2$. Check this out for yourself on some small sets of numbers (e.g. 1,2,3...). Intuitively, then, if all the measurements from Sample 1 are much smaller than those from Sample 2, then the minimum sum of ranks for Sample 1 would be $R_1 = n_1(n_1+1)/2$. 

#### Computing the U-statistic

The $U$_statistic measures the difference between the observed and minimum ranks. Note that the sum of the $U$-values equals the product of the length of the two samples: $U_1 + U_2 = n_1n_2$.

We can use a simplified method to compute the $U$-statistics:

$$ U_1 = R_1 - \frac{n_1(n_1+1)}{2} = R_1 - minR_1$$
$$U_2 = R_2 - \frac{n_2(n_2+1)}{2} = n_1n_2 - R_1$$
where $n_{1}$ and $n_{2}$ are the size of the two groups.

The population mean and variance (again, ignoring the term for ties for now) are:

$$ \mu_U = \frac{n_1n_2}{2}\ ; \ \sigma_{U}^2 = \frac{n_{1}n_{2}}{12} (n_1 + n_2 + 1)  $$

Below we will perform the Mann-Whitney-Wilcoxon test on the triglyceride data, pretending that they are not actually paired. First, we compute the statistics by hand:

```{r}

n = length(pre)  # n is the same for both sets
r_min = n*(n+1)/2 # minimum rank

Data = c(log(pre),log(post))
DataRank = rank(abs(Data))
DataRank

DataRankSums = tapply(DataRank,rep(c("pre","post"),each=n),sum)
DataRankSums

t_pre  = DataRankSums["pre"]
t_post  = DataRankSums["post"]

w_pre = t_pre - r_min
w_pre
w_post = t_post - r_min
w_post

mu_w = n^2 / 2
mu_w

sigma_w = (n^2/12) * (2*n + 1)
sd_w = sqrt(sigma_w)

```

#### Normal approximation

Again, when the number of samples is ~20 or more, we can use the normal approximation. We have $n_1 = n_2 = 24$, so let's just calculate a $z$-score get our $p$-value.

```{r}
z_w = (w_post - mu_w)/sd_w
z_w

# these are equivalent
2*pnorm(z_w)
2*pnorm(w_post, mean=mu_w, sd=sd_w)
```

Check the above calculations using the `wilcox.test()` function. Try running the test on both the raw and the log-transformed data.

```{r}
# raw data
wilcox.test(post,pre, paired=FALSE)

# log-transformed data
wilcox.test(log(post),log(pre), paired=FALSE)
```

How do the manual results compare with the results from the R functions? Why are they not identical?

<!-- YOUR ANSWER HERE -->

How do the results from the raw and transformed data compare?

<!-- YOUR ANSWER HERE -->

## Conclusions

Finally, please reflect on the results from the different tests we have performed here. Summarize in a short paragraph below what you have learned from the analysis of the triglyceride dataset.

<!-- YOUR ANSWER HERE -->
