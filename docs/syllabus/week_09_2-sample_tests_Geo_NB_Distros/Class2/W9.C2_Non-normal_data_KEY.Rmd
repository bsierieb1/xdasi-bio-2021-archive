---
title: "Tests for violations of normality"
subtitle: "XDASI Fall 2021"
date: "10/28/2021"
output:
  pdf_document:
    toc: yes
    toc_depth: 4
  html_document:
    toc: yes
    toc_float: yes
    toc_depth: 4
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Is it ok to perform a $t$-test on my data?

So far we have used $t$-tests to compare two samples. These depend on the assumption that data are normally distributed. How do we know if this is the case?

Let's return to our simple case study where a drug has been provided to 10 random patients (test subjects) and, as a control, a placebo pill was given to 10 other random patients. Measurements were collected for each condition and the question is:

***Is there a significant difference between the subjects who were given the placebo and the those who were given the drug?***

Let's first look at all the data together as a population. We will combine the values and draw a simple histogram.

```{r}

Placebo = c(54,51,58,44,55,52,42,47,58,46)
Drug = c(54,73,53,70,73,68,52,65,65,60)

Data = c(Placebo, Drug)
Data_sd = sd(Data)
Data_mean = mean(Data)
hist(Data, breaks=10)
```

Do these data look "sufficiently normal" to perform regular parametric tests on them? How can we tell?


## Quantile-Quantile plots

### Normal QQ plot

We can check graphically to see how well an observed set of values matches a theoretical normal distribution using the `qqnorm` and `qqline` functions.

The theoretical distribution is generated using the same number of datapoints as the sample data, but distributed evenly across probability quantiles for a standard normal distribution. The corresponding $z$-scores are then plotted against the sample data.

The closer the observed data points fall on the "ideal" line, the more closely they resemble normally distributed data. Of course, due to sampling variation, the actual data will usually not fall exactly on the line, but random samples drawn from a normal distribution should follow the line at least approximately.

```{r}
qqnorm(Data)
qqline(Data)
```

### More general QQ plot

A more generic function is `qqplot` function, which can be used to compare data to theoretical values based on any distribution we want. To make such a plot requires a few steps:

#### `ppoints`

The `ppoints` function takes the number of values you want, which should match the length of the dataset you want to test. The function returns the same number of equally spaced quantiles for the cumulative probability between 0 and 1. 

We can then use the `qnorm` function to get the predicted $z$-scores of the theoretical normal distribution for the corresponding quantiles.

```{r}
my_probs = ppoints(length(Data))
my_probs
my_quant = qnorm(my_probs)
my_quant
```

#### `quantile`

Now to plot the corresponding values from the observed data distribution, we can use the `quantile` function. 

In addition to the sample data, the `quantile` function also takes the (evenly spaced) cumulative probabilites that we have stored in `my_probs`, which it will use to infer values based on the observed data.

```{r}
data_quant = quantile(Data, my_probs)
data_quant
```

#### `qqplot`

When we now plot these against each other, we get a very similar plot to what we saw with `qqnorm`, except now we are using imputed values based on the input data.

Again, values based on the observed data are plotted on the y-axis, and normalized values for the the theoretical data are plotted on the x-axis. 

For normally distributed data, `qqnorm` and `qqplot` should give very similar results (though they will usually not be exactly the same since `qqnorm` goes through an extra imputation step). Both are shown below for easier comparison.

```{r}
par(mfrow = c(1,2))

## qqnorm plot
qqnorm(Data)
qqline(Data)

## qqplot plot
qqplot(my_quant, data_quant, 
       xlab="Theoretical quantiles", ylab="Quantiles inferred from data",
       main="General Q-Q Plot")
qqline(Data)
```


## Shapiro test for normality

The Shapiro test performs a **goodness of fit** test using the mean and standard deviation of the data. The null hypothesis is that the data are normally distributed.

```{r}
shapiro.test(Data)
```

## Data transformation

In our example, there is not enough evidence to reject the $H_o$ that the data is normally distributed. So let's look at a different dataset. The R dataset `trees` provides `Girth`, `Height`, and `Volume` of different Black Cherry Trees. Let's look at the histogram of the volume data:

```{r}
hist(trees$Volume)
```

Hm. This dataset doesn't look like it is normally distributed. Performing a `shapiro.test` confirms this.

```{r}
shapiro.test(trees$Volume)
```

And the `QQ plot`:

```{r}
qqnorm(trees$Volume)
qqline(trees$Volume)
```

We can see from the Shapiro test and the QQplot that these data don't seem to fit a normal distribution. In a QQplot, this is usually quite obvious because parts of the data veer pretty far off from the ideal line.

When the data do not look to be sufficiently normal, we can try performing tests on data that have been **transformed** using functions such as the `log()`, `sqrt()`, or `arcsin()`. This can make data look more normal, so that parametric tests may be performed on them.

### Log transform

In biology it is common that multiple factors influence our measurements. The effects may be additive or multiplicative. We know from probability theory that to find the cumulative probability of several independent variables, we can multiply them (product rule). This type of data often gives rise to log-distributed measurements. Taking the log of these stretches out the small values and compresses the larger ones, rendering the data more normal-looking.

Many examples follow a log-normal distribution, such as exponential growth (cell count doubles with each division), systolic blood presssure, and the latency period for infectious diseases.

How does the `trees` dataset look when we log-transform it?

```{r}
par(mfrow=c(1,2))

qqnorm(trees$Volume)
qqline(trees$Volume)

qqnorm(log(trees$Volume))
qqline(log(trees$Volume))

shapiro.test(log(trees$Volume))
```

Not perfect, but certainly closer to normality than the original data.


## Example: Plasma triglyceride levels

Let's investigate the dataset below, containing measurements for plasma triglyceride levels in test subjects before and after changes in their diet and exercise programs. If there is any significant difference, we expect to see a decrease in triglyceride levels with these lifestyle changes.

We will use the following methods to examine the data:

+ histograms
+ QQ plots
+ Shapiro-Wilk test for normality
+ a `t.test()` on the transformed data
+ the 95% CI for the data in their original units
  - we can compute by hand (hard), or extract from `t.test()$conf.int[1:2]` (easy)
  - don't forget to back-transform using `exp()`

```{r, collapse=TRUE}
#####################################################################
# plasma triglyceride levels in the population (mg/ml)
# borderline high = 2-4 vs. normal < 2
# testing before and after diet and exercise changes (expect a decrease)
pre = c(2.55,3.38,2.37,4.11,3.27,2.58,4.20,3.22,5.10,2.62,3.06,1.23,2.27,2.24,1.39,2.63,2.61,4.30,1.46,3.35,2.79,2.42,4.63,1.57)
post = c(1.59,3.51,1.44,2.32,1.75,1.67,1.90,1.37,2.72,1.80,2.40,2.01,2.41,1.38,1.18,4.31,2.09,2.32,2.63,2.25,1.71,2.01,1.95,3.46)
#####################################################################
```


### Histograms and QQ plots

Let's take a quick look at the original and log-transformed data:

*Note: By default, the `log()` function in R uses the natural log. You can specify other bases with the `base` parameter; convenience functions such as `log10()` and `log2()` are also available.*

```{r, collapse=TRUE}

# check distributions of original data with histograms (breaks = 10)
par(mfrow=c(2,2))
hist(pre,breaks=10)
hist(post,breaks=10)

# check distributions after log-transformation
hist(log(pre),breaks=10)
hist(log(post),breaks=10)
```

```{r}
par(mfrow=c(2,2))
qqnorm(pre)
qqline(pre)

qqnorm(post)
qqline(post)

qqnorm(log(pre))
qqline(log(pre))

qqnorm(log(post))
qqline(log(post))

```


### Shapiro-Wilk test for normality

Do these samples look approximately normally distributed?

```{r}
# Shapiro-Wilk tests
shapiro.test(pre)
shapiro.test(post)

# Shapiro-Wilk tests
shapiro.test(log(pre))
shapiro.test(log(post))

```


What do you conclude from inspection of the normality of the data?

```{r eval=FALSE}
# the "post" data are not normally distributed
# log transformation makes it more normal, but the "pre" group gets a bit worse

# however according to the Shapiro-Wilk test, both are within acceptable limits
# for continuing on with a t-test on the log-transformed data
```


### $t$-tests

Let's compare the results of $t$-tests using the original and the transformed data.

```{r, collapse=TRUE}

## do t-tests using the original and transformed data
t.test(post,pre,paired=T)
t.test(log(post),log(pre),paired=T)
```

What are the most striking differences in the results of the $t$-tests? Which $t$-test is more appropriate, and why?

```{r eval=FALSE}
# results are actually similar, though of course the t-stats, CIs, and mean differences
# are not the same

# the transformed test is more correct based on normality tests above
```


### 95% confidence intervals

Compute the 95% CI for the post-treatment and pre-treatment samples using the log-transformed data.

#### Manual calculations

First, do these "by hand" using the formulas you know for the 95%CI (don't forget to back-transform into the original units!).

```{r, collapse=TRUE}
# ============================================================================ #
# pre-treatment -- by hand!

# compute standard error
mean_prime = mean(log(pre))
sd_prime = sqrt( sum( (log(pre) - mean_prime)^2 /(length(pre)-1)) )
se_prime = sd_prime/sqrt(length(pre))

# get t-critical
tcrit = qt(0.975,df=length(pre))
tcrit

# CI in log units
ci_pre = c(mean_prime - tcrit*se_prime, mean_prime + tcrit*se_prime)
ci_pre

# CI in original units
ci_pre_orig = exp(ci_pre)
ci_pre_orig


# ============================================================================ #
# 95%CI post-treatment -- by hand!
mean_prime = mean(log(post))
sd_prime = sqrt( sum( (log(post) - mean_prime)^2 /(length(post)-1)) )
se_prime = sd_prime/sqrt(length(post))
tcrit = qt(0.975,df=length(post))
tcrit

ci_post = c(mean_prime - tcrit*se_prime, mean_prime + tcrit*se_prime)
ci_post
ci_post_orig = exp(ci_post)
ci_post_orig
```


#### Extracted from $t$-test output

Now use the CI values from one-sample $t$-tests to get both of these automatically (again, don't forget to back-transform!) To get these, simply perform 1-sample tests for each group against a group mean.

(Note: these tests should not be used for computing p-values, but the CI for each group is given in the output.)

```{r}
# ============================================================================ #
# 95%CI pre-treatment from t-test
t_pre = t.test(log(pre),mu=mean(log(pre)))
t_pre
ci95_pre_log = t_pre$conf.int[1:2]
ci95_pre_log
ci95_pre = exp(ci95_pre_log)
ci95_pre

# ============================================================================ #
# 95%CI post-treatment from t-test
t_post = t.test(log(post),mu=mean(log(pre)))
t_post
ci95_post_log = t_post$conf.int[1:2]
ci95_post = exp(ci95_post_log)
ci95_post_log
ci95_post
```

Did you get the same CI for the post-treatment sample when computing by hand and using the `t.test()`? What can you conclude from the 95% CI's of the two samples?

```{r eval=FALSE}
# yes, the CIs are exactly the same whether computed by hand or extracted from a t-test.

# the CI's for the two samples barely overlap so the groups are most likely significantly different. The p-value for the paired two-sample test performed previously confirmed this.
```
