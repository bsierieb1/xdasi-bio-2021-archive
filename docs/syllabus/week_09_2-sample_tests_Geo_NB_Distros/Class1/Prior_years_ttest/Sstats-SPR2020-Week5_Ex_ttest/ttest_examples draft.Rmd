---
title: "T-test"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Bootstrapping: sampling with replacement

Here will discuss a simple case study where the a drug has been provided to 10 random patients (or test subjects) and, as a control, a placebo pill was given to 10 other random patients. Measurements were collected for each condition to answer the question:

### Is there a significant difference between the control subjects and the those who were given the drug?

### The Population

Let's first look at all the values for both groups as a population. We will combine the values and draw a simple histogram.

```{r}

Placebo = c(54,51,58,44,55,52,42,47,58,46)
Drug = c(54,73,53,70,73,68,52,65,65,60)

Data = c(Placebo, Drug)
Data_sd = sd(Data)
hist(Data)

```

### Estimating with the bootstrap

**Bootstrapping** is when we sample our dataset **with replacement**. The idea is that we sample repeatedly from the same data values to approximate what the population may look like. 

Let's simulate randomly obtaining 10 measurements from the sample. If we do this 10,000 times, we will see that the *distribution of the sample means* of the simulated population follows a normal distribution. 

In fact, the standard deviation of this distribution is called the **Standard Error (SE)**.

```{r}
P_sample=numeric()
n=10
for (i in 1:10000) {
  P_sample[i] = mean(Placebo[sample(1:length(Placebo), n, replace=T)])
}

hist(P_sample)
sd(P_sample)

```

Try this for n=5, n=10, and n=50. Notice that the SE decreases as the number of samples (n) increases. This relationship can be described as:

$$SE_{\bar{x}} = {\frac{\sigma}{\sqrt{n}}}$$

```{r}
P_sample=numeric()
n=5
for (i in 1:10000) {
  P_sample[i] = mean(Placebo[sample(1:length(Placebo), n, replace=T)])
}

hist(P_sample)
sd(P_sample)


```

```{r}
P_sample=numeric()
n=50
for (i in 1:10000) {
  P_sample[i] = mean(Placebo[sample(1:length(Placebo), n, replace=T)])
}

hist(P_sample)
sd(P_sample)


```
### Confidence Interval

The standard error is very helpful because it gives us an idea of how close we are to the actual mean. We can use the SE to help define **Confidence Intervals (CI)** of what the the actual population mean is.

Let's take our Placebo sample and try to calculate our 95% CI for the mean of the population. In this case, let's pretend we don't know the population, and Placebo is just a sample from some unknown population. Since we don't know the true $\sigma$, we will estimate it by calculating the $sd$ of the sample. Note that this only works if the population is truly normally distributed. 

Here, the estimated distribution of the sample means will follow a $t$-distribution, which is similar to a standard normal distribution but with heavier tails. As the sample size increases, the tails of the $t$-distribution become smaller and it resembles closely the $Z$-distribution.

We can use the `qt()` *quantile function* to identify the $t$-score where $df$ is one less than the number of samples and the area under the curve is 97.5%.

We then add and subtract this amount from the estimated mean to get our 95% CI.

```{r}
Pse=sd(Placebo)/sqrt(length(Placebo))
Pse  # 1.8 -- this is one SD from the sample mean

t_0.975 = qt(p = .975, df = length(Placebo)-1)
t_0.975  # 2.26 = z-score for 97.5% area

# now we can compute
# point on x-axis in original distribution that corresponds to z = 2.26
Pse * t_0.975
# now can calculate 95% CI
Pmean = mean(Placebo)
Pmean - Pse * t_0.975
Pmean + Pse * t_0.975

# use sample SD to approximate population SD
# now can get an estimate how how likely it is that we have captured the true pop mean
```

We can check our answer by using the `t.test()` function, providing $\bar{x}$ since we don't know the $\mu$ (the true population mean).

```{r}
t.test(Placebo, mu=mean(Placebo))
```

### Known $\mu$ - One sample t-test

If we do know $\mu$, then we test to see how likely it is that our sample comes from this population. We first calculte the $t$ statistic by seeing how many SE the Placebo mean is from the population mean. Then we can look to see the area of under the curve for that statistic. Since we are considering both extremes, we should multiply the $p$ value by 2.

```{r}
(Pmean - mean(Data))/Pse

# Because we want to look at both extreme values (lower 2.5% and higher 2.5%),
# and we know the distribution is symmterical, simply multiply p-value by 2.
pt((Pmean - mean(Data))/Pse, df = 9)*2

```

#### Null hypothesis

Our null hypothesis in this case is that there is no difference between Placebo mean and Population mean. If this was true, our T-statistics would be low. But our T-statistic was high, giving us a p value of less than 0.05 (the standard cutoff for considering something significant). Therefore we consider the alternative hypothesis: that the Placebo group is significantly higher or lower than the control Population.

We compute this by considering the **difference between the means**:

```{r}
t.test(Placebo, mu=mean(Data))

```

### Difference between sample means - two-sample $t$-test


Now let's start to consider that Placebo and Drug groups really are two separate populations. If we randomly sample from the two different populations, we can estimate (with 95% confidence) the true difference in the means of the two populations.

We already know that the distribution of the sample mean is normally distributed, and that the sum or difference of two normally distributed variables is also normally distributed. Therefore, the difference in sample means must be normally distributed.

Below I will sample 5 values from both populations many tines, sinply taking their difference each time:

```{r}

Diff_sample = numeric()
n=5
for (i in 1:10000) {
  Diff_sample[i] = mean(Placebo[sample(1:length(Placebo),n,replace=T)])-
    mean(Drug[sample(1:length(Drug), n, replace = T)])
}

hist(Diff_sample)

```

When we consider the means of two samples, it turns out the SE of the difference is the square root of the sum of the SE of each sample:

$$\sigma_{\mu_{1}-\mu_{2}}={\sqrt{\frac{\sigma_{1}^2}{n_1}+{\frac{\sigma_{2}^2}{n_2}}}}$$

```{r}
se_diff = ( sqrt (
  ((sd(Placebo)**2)/length(Placebo)) + 
    ((sd(Drug)**2)/length(Drug))))

se_diff

```

#### 95% CI

We can caculate the 95% confidence interval by using the $t$-distribution. In this case, the degrees of freedom is the sum of the total values minus the number of samples ($df = 20-2$).

```{r}

Diff_mean = mean(Placebo) - mean(Drug)  # -12.6
Diff_mean

qt(p=0.025, df=18)*se_diff
qt(p=0.975, df=18)*se_diff

Diff_mean + qt(p=0.025, df=18)*se_diff
Diff_mean - qt(p=0.025, df=18)*se_diff

Diff_mean - qt(p=0.975, df=18)*se_diff
Diff_mean + qt(p=0.975, df=18)*se_diff

Diff_mean + qt(p=0.025, df=18)*se_diff
Diff_mean + qt(p=0.975, df=18)*se_diff
```

Notice that the confidence interval does not span 0, which means it is extremely unlikely that the true difference between the Drug and Placebo populations is 0.

Let's check is this is true.

We can look at the T-statistics ( which is the equivalent to the Z-score if it was normally distributed ) we simply divide the difference of the sample by the size of SE.

```{r}

tstat = Diff_mean / se_diff
tstat
```

#### Two-sample $t$-test

Now we can calculate the p-value that our null hypothesis is true. The null hypothesis is that the difference between the two populations is 0. We know this because if we were to plot a distribution from random variables with the same mean, we would expect to get a difference of 0.

```{r}
#using our calculations done by hand
2*pt(q = tstat, 
     df = (length(Placebo)+length(Drug)-2), 
     lower.tail = T)

#confirmation using the t.test() function.
t.test(Placebo, Drug, var.equal = T)
```

#### Paired $t$-test

What if the data are taken from the **same individuals**? For example, we could test the same patients before and after treatment. In this kind of experimental design, we say that the data are **paired**. If there is no difference between the two measurements for each individual -- for example, a new drug for blood pressure has no measurable benefit -- then we would expect that our before and after values would be about the same on average. 

The **paired $t$-test** is performed in the same way as the one-sample $t$-test, except we use the **mean difference between paired measurements** from the two samples, $\bar{X}_D$, to compute a test statistic. Our **null hypothesis** is usually that the mean difference, $D_o$, is zero (we could set it to something else if our null hypothesis is that the difference between them is something else ...).

For paired data, we assume that the two sets of measurements are arranged in the same order as the corresponding individuals (because we have good record-keeping practices!) The $t$-statistic is:

$$ t^* = \frac{\bar{X}_{D} - D_0}{\frac{s_{D}}{\sqrt{n}}} = \frac{\bar{X}_{D} - D_0}{SE_{D}}  = \frac{\sqrt{n}}{s_D}(\bar{X}_{D} - D_0) $$

where $\bar{X_D}$ is the mean of the pairwise differences, $s_D$ is the standard deviation of the pairwise differences, and $D_0$ is what we are testing (which in this case is $0$).

To compute the test statistic, we can simply subtract one vector from the other to obtain pair-wise differences for each individual, take the mean, and divide by the standard error. We then find the $p$ value in the usual way.

```{r}
DrugDiff = Placebo-Drug

t_stat = mean(DrugDiff)/(sd(DrugDiff)/sqrt(length(DrugDiff)))

2*pt(t_stat, df=9, lower.tail = T)
```

```{r}
t.test(DrugDiff, mu=0)
```

We can also confirm that the one sample $t$-test is same as a $t$-test with the paired option:

```{r}
t.test(Placebo, Drug, paired = T, var.equal = T)

```


## Non-parametric tests

What if the data are not normally distributed? When we cannot assume that this is the case, we can use **rank-based** tests to see if our samples are statistically different. These are called **non-parametric** tests because they do not assume that the data follow any distribution that can be described mathematically by specifying known parameters (like the normal, log-normal, binomial, Poisson, etc.). Non-parametric tests have less **power** than parametric tests, but often the results are very similar.

### Paired data - Wilcoxon signed-rank test

Continuing with the example above, sometime we have paired data, but we cannot assume that the samples come from a normal distribution. Instead of looking at the actual magnitudes of the differences between pairs of measurements, we will compare the **ranks** of the values from the two groups. 

The **Wilcoxon signed-rank test** is a non-parametric equivalent of a **paired $t$-test**. Our null hypothesis is that the *sum of the ranks* from one group is the same as that from the other group when we combine all the data together. This makes intuitive sense because if we shuffle a deck of cards, they should be in random order, so we would not expect to find all of the cards from one suit (or all the royal cards) to appear at the beginning or end of the deck.

The steps are:

+ Calculate the differences between the pairs
+ Count the number $n$ of nonzero differences
+ Order the *absolute* values of the $n$ differences from lowest to highest (assign the average if there is a tie)
+ Sum up the ranks of all the positive differences and all the negative differences separately
+ The lower **total sum of ranks** (positive or negative) is the test statistic, $W$.

Under the **null hypothesis**, $W$ follows a distribution with mean equal to 0 and a variance equal to:

$$ \sigma_{W}^2 = \frac{n(n+1)(2n+1)}{24} $$

For large $n$ (> ~20), the sampling distribution of $W$ approximates a normal distribution, so it's possible to compute a $Z$-score, where:

$$ z = \frac{W}{\sigma_W} $$.


~~~~~~

Let's compute the test statistic by hand:

```{r}
Data

DrugDiff = Placebo - Drug
print(DrugDiff)

DrugDiff = DrugDiff[which(DrugDiff != 0)]
print(DrugDiff)

DrugDiffRank = rank(abs(DrugDiff))
print(DrugDiffRank)

t_neg = sum(DrugDiffRank[DrugDiff < 0])
t_neg

t_pos = sum(DrugDiffRank[DrugDiff > 0])
t_pos

t_min = min(t_pos,t_neg)
t_min

n_diff = length(DrugDiff)
n_diff
```


The population mean for the statistic is:

$$ \mu_{v} = \frac{n(n+1)}{4}$$

Since for us the $n$ was 9 (we had one zero difference), 

$$ \mu_{v} = \frac{9(10)}{4} = 22.5$$
```{r}
mu_diff = n_diff*(n_diff+1)/4
mu_diff
```

The population is a bit complicated for ties, so let's just focus on the case where there are no ties.

$$ \sigma_{V}^2 = \frac{n(n+1)(2n+1)}{24} $$

```{r}
var_diff = n_diff*(n_diff + 1)*(2*n_diff + 1)/24
var_diff
```

To get the standard deviation we simply take the square root.

```{r}
sd_diff = sqrt(var_diff)
sd_diff
```

When there are plenty of samples (usually > 25), a normal approximation is used to estimate the $p$ value. Let's just assume we do and calculate the *z-statistic* to determine our $p$ value:

```{r}
z_diff = (t_min - mu_diff)/sd_diff
z_diff

2*pnorm(z_diff, lower.tail = T)

```

Let's confirm this

```{r}
wilcox.test(Placebo, Drug, paired = T)

```

####  

What if our data are not normally distributed but also not paired?

We can use the **Wilcoxon rank sum test** but instead of separating them by positive and negative, we will separate them by their group

The steps are as follows:

+ Combine the values and rank them together.

+ Rank them (smallest to largest where smallest gets a 1)

+ If there is a tie, provide the average as the rank for all.

+ $T_{1}$ is the sum of the rank group one and $T_{2}$ is the sum of the ranks 
in group two.

+ Calculate W using the formulas below:

$$ W_{1} = T_{1} - \frac{n_{1}(n_{1}+1)}{2} $$
$$ W_{2} = T_{2} - \frac{n_{2}(n_{2}+1)}{2} $$
where $n_{1}$ and $n_{2}$ are the size of the two groups.

```{r}
Data
DataRank = rank(abs(Data))
DataRank

DataRankSums = tapply(DataRank,rep(c("P","D"),each=10),sum)
DataRankSums

n_placebo = length(Placebo)
n_drug = length(Drug)

t_P  = DataRankSums["P"]
t_D  = DataRankSums["D"]

w_P = t_P - (n_placebo*(n_placebo + 1)/2)
w_P
w_D = t_D - (n_drug*(n_drug + 1)/2)
w_D
```

The population variance ( again, ignoring the term for ties for now ) is 

$$ \sigma_{W}^2 = \frac{n_{1}n_{2}}{12} (n_1 + n_2 + 1)  $$
```{r}

sd_w_squared = (n_placebo*n_drug/12) * (n_placebo + n_drug + 1)
sd_w = sqrt(sd_w_squared)
sd_w
```

And the population mean is :

$$ \mu_W = \frac{n_1n_2}{2}$$

```{r}
mu_w = n_placebo * n_drug / 2
mu_w
```

Again, with high number of samples we can approximate to normal distribution, so let's just calculate the z score and get our p-values.

```{r}
z_w = (w_D-mu_w)/sd_w
z_w

2*pnorm(z_w, lower.tail = F)
2*pnorm(w_D, mean=mu_w, sd=sd_w, lower.tail = F)
```

Confirmation that our calculation is correct

```{r}
wilcox.test(Placebo, Drug)

```

### Exercise

#### Perform a bootstrap analysis of non-parametric methods

+ Perform a permutation analysis demonstrating the mean of the $W$ statistic in two group wilcoxon test is correct.

```{r}
# Break the problem into discrete steps

# Combine the values and rank them together.

#Placebo = c(54,51,58,44,55,52,42,47,58,46)
#Drug = c(54,73,53,70,73,68,52,65,65,60)
#Data = c(Placebo, Drug)
Data

# create a factor
DataGroup = factor(rep(c("P","D"),each=10))
  
# Create a loop that repeates 10,000 times
for (i in 1:10000) {

  # shuffle values
  shuffled.values = sample(Data, replace=F)

  # rank shuffled values (smallest to largest, with smallest = 1)
  DataRank = rank(abs(Data))
  DataRankSums = tapply(DataRank,DataGroup,sum)

  # calculate T1 and T2
  t_P  = DataRankSums["P"]
  t_D  = DataRankSums["D"]

  # calculate W1 and W2
  n_placebo = length(Placebo)
  n_drug = length(Drug)

  w_P = t_P - (n_placebo*(n_placebo + 1)/2)
  w_D = t_D - (n_drug*(n_drug + 1)/2)

  # take the min(W1,W2) and save it in a growing vector
  min(w_P,w_D)
}

```

+ Perform a bootstrap analysis to show the standard deviation of the $W$ statistic in the two group wilcoxon test is correct.


